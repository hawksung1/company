# Ensemble Model

## 1. 소개

### 1.1 정의

  - 단순다수결 : 배깅, 랜덤포레스트에서 사용.
  - 가중다수결 : 부스팅에서 사용(가중치 다수결)
* 좀 더 robust한, 예측모형/분류모형 생성 및 feature selection(중요변수선택)을 위해 사용함
* 분류모형, 예측모형에 모두 사용가능함. 일반적으로 분류모형에 사용됨.

### 1.2. 앙상블모형의 성공요인 :: "Diversity"의 확보

* 앙상블 집합 내에 구축되는 모형이 서로 유사하지 않고 매우 다양해야 함.

* 다양한 B개의 분류기를 통합해야 데이터 자체에 존재하는 복잡/난해한 구조의 분류문제를 해결할 수 있다. 

* 각각의 샘플 데이터마다 반드시 똑같은 방식의 모형을 쓸 필요는 없음

  #### (1) 데이터의 다양성 :: Bootstrap

  * 하나의 데이터를 이용해 다양한 데이터를 만들어 내는 방법
  * 훈련데이터 전체가 L이라고 한다면, 부트스트랩 방법은 L1, L2 ..., Lb개의 데이터를 생성

  #### (2) 모형생성의 다양성 :: 분할방법

  * 각각의 훈련데이터마다 다양한 모델을 적용하는 방법도 있으나, 보통은 Decision Tree를 사용하되, Model 적합시 분할방법에 변화를 주어 모형이 다양해지도록 함.
  * 랜포의 경우 중간 노드에서 분할후보점을 선발할 때 전체변수가 아니라 임의의 변수 부분집합 중에서 선발하고, 그 중에서 분할개선도를 최대화시키는 분할 point를 탐색함.
  * 동일한 모형생성 방식을 사용하더라도 하이퍼파라미터 튜닝을 한다면 모형생성의 다양성을 확보하는 한가지 방법이 될 수 있음 

## 2. 종류

### 2.1 Bagging :: Bootstrap Aggregating

* (1) 먼저 Train Data $$L$$을 뽑는다. (ex. 전체 데이터의 70%) 
   :: $$ L=\{(x_i,y_i), i = 1, ..., n\} $$ 

  (2) $$L$$로부터 복원추출하여 부트스트랩 데이터를 B번 만든다 (일반적으로 B로 50을 많이 사용. )

   :: $$L_1, L_2, ... , L_B$$
  *** 데이터를 복원추출 시 B의 횟수를 아무리 늘려도 전체 데이터의 63.2%만 추출이 됨.
  왜 63.2%인지. 자세한 내용은 검색을 통해 확인가능(keyword : bootstrap 0.632)

  (3) 각 부트스트랩 데이터마다 모델(분류기)를 생성
  :: $$T_1, T_2, ... , T_B $$ - 여기서 T는 Decision Tree

  (4) B개의 모델을 결합시켜 최종 앙상블모델을 생성. 
  :: 범주형 모델(분류기)인 경우에는 다수결 방식에 따라 결정.
  :: 연속형 모델(Regression)인 경우에는 평균값을 사용

* 배깅방법은 데이터가 쪼금만 변해도 결과에 많은 변화가 생기는 불안정한 분류모델인 경우,
   예측력을 획기적으로 향상시킨다고 알려져 있음.
  -> Decision Tree의 경우 pruning(가지치기)를 사용하지 않는 최대 Tree를 사용하는 것이 예측정확도가 좋다고 알려짐 (가지치기를 사용하지 않은 경우에 더 불안정한 모델이므로)

* <mark>Bagging 적용하면 안 되는 경우
  1) Too small : 표본데이터가 작은 경우 전체를 잘 반영하지 못함..
  2) Noise : 데이터에 noise가 많은 경우(ex. outlier가 크게 왜곡시킴)
  3) Dependency : 데이터의 독립성 가정

### 2.2 Boosting

(https://www.youtube.com/watch?v=GM3CDQfQ4sw)

- 배깅과 마찬가지로 B개의 분류기를 생성하여 종합하는 방법이지만, 분류기를 생성하는 방식과 종합하는 방식이 조금 다르다
- 무작위로 선택하는 것보다 약간 가능성이 높은 규칙(weak learner/classifier)들을 결합시켜 보다 정확한 예측 모델을 만들어 내는 것을 말함. 
- 부스팅에 사용되는 분류기는 오분류율 기준으로 랜덤하게 예측하는 것보다 약간이라도 좋은 예측 Model 이기만 하면 효과가 있다고 알려짐. ==> 예측력이 약한 분류모델을 결합하여 강한 예측 모델을 만드는 과정!
- Adaboost 알고리즘( Adaptive Boosting)을 주로 사용함
- Boosting은 새로운 learner(classifier)를 학습할 때마다 바로 이전 결과를 참조하는 방식.
  최종적으로 weak learner로부터의 출력을 결합하여 더 좋은 예측율을 갖는 strong learner를 만들어 냄.

_____ Boosting 사례) (이 부분은 '쉽게 읽는 머신 러닝(https://blog.naver.com/laonple)'에서 참고.)

- 예시) 가령 스팸 여부를 가릴 수 있는 방법을 개발한다고 해보자. 스팸 여부 판정할 수 있는 기준은 많으며, 다음과 같은 것들이 있다고 하자. (간단한 규칙들에 대하여 Yes/No로만 판정)

  - 링크만 있는 경우 => 스팸
  - 메일 내용에 “당신의 보험료가 xxx 입니다”라는 내용이 들어 있는 경우 => 스팸
  - 도메인 주소가 확실한 경우 => 스팸 아님
  - 보낸 사람이 확실한 경우 => 스팸 아님

  스팸 여부를 판단할 수 있는 기준들은 위에서 열거한 일부 경우뿐만 아니라 매우 많은 기준이 있을 수 있다. 그런데 위에서 열거한 기준들은 모든 메일에 똑같이 적용할 수 있을 정도로 확실한/강력한 규칙은 아니다. 이런 것들을 weak learner(rule, classifier)라고 한다. 무작위로 선정하는 것보다는 성공 확률이 높은, 다시 말하면 오차율이 50% 이하인 학습 규칙을 말한다


  Weak learner를 strong learner로 바꿀 수 있는 방법은 무엇일까? 간단한 방법은 다음과 같다.

  - 평균/ 가중 평균(weighted average)를 사용하는 방법
  - 가장 많은 의견(vote)을 얻은 것을 선정하는 방법
    
    

  앞서 살펴본 스팸 메일 여부를 가리는 규칙에 위 두가지 방법을 적용하면, 좀 더 강력한 스팸 필터가 만들어질 것이며, 규칙이 더 늘어나면 늘어날수록 더 좋은 결과를 기대할 수 있게 된다.

### 2.3 <mark>Bagging vs. Boosting

- Bagging은 모든 Bootstrap이 서로 독립적인 관계를 갖는다. 하지만 Boosting은 순차적으로 처리가 되며, 에러가 발생하면 그 에러의 weighting을 올리기 때문에 현재의 weak learner가 이전 weak learner의 영향을 받는다.
- Boosting은 최종적으로 weighted vote를 하지만, Bagging은 단순 vote를 한다.
- Bagging은 Variance를 줄이는 것이 주 목적이지만, Boosting은 Bias를 줄이는 것이 주 목적이다. 
- 잡음이 없는 데이터에 대해서는 Boosting이 Bagging보다 우수하다.
- Bagging은 overfitting 문제를 해결할 수 있지만, Boosting은 overfitting 문제로부터 자유롭지 못하다