​- 11.1, 11.2, 11.3

 

 

11.1 그레이디언트 소실과 폭주 문제

 

그레이디언트 소실

    정의

        알고리즘이 하위층으로 갈수록 그레이디언트가 점점 작아져서 결국 소실되는 문제

그레이디언트 폭주

    정의

        위와 반대로 점점 커져서 발산하는 문제

--> 위의 두 문제가 신경망을 학습하기 힘들게 만든다.

 

방지법

    1. 글로럿(세이비어) 초기화

        각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 한다.

        역방향에서 층을 통과하기 전과 후의 그레디언트 분산도 동일해야한다.

        방법

            가중치를 글로럿 초기화함수를 이용해 무작위 초기화

        단점

            fanin = fanout 하지 않으면 잘 적용 안된다.

            ReLU 에 잘 안맞음.

    2. 르쿤 초기화

        위의 첫번째 단점을 개선

    3. He 초기화

        ReLU 활성화 함수에 대한 초기화 방법

사용법

    Keras

        default

            글로럿 초기화 사용

        변경법

            kernel_initializer 변경

        예시

            keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal)

ReLU 함수의 문제점

    뉴런이 0 값만 출력하는 "죽은 ReLU" 문제

    해결

        1. LeakyReLU

            장: 항상 ReLU보다 성능이 좋음

        2. RReLU

            장:  train 과대적합 위험을 줄여줌

        3. PReLU

            장: 대규모 이미지데이터에서 ReLU보다 성능이 좋음

            단: 소규모에서 과대적합



        4. ELU

            장: ReLU 시리즈보다 성능이 좋음

            단: 계산이 느림.

        5. SELU

            장: 그레디언트 소실, 폭주문제 방지

            단: 사전 조건

                1. 입력이 반드시 표준화

                2. 모든 은닉층 가중치 르쿤 정규분포 초기화

                3. 순환망, 스킵연결 사용 X



배치 정규화

    정의

        활성함수 통과전 또는 후에 입력을 원점에 맞추고 정규화 후 2개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동시키는 함수 추가.

    팁

        첫번째 층으로 사용하면 입력값을 따로 정규화할 필요가 없음.

    TEST 방법

        train 에서 입력 평균과 표준편차를 이동 평균을 이용해 구해 놓는다.

    장

        뛰어난 정확도

        다른 추가적인 규제(드롭아웃 등)의 필요성을 줄여줌.

    단

        실행 속도 저하

그레디언트 클리핑

    그레디언트 폭주 문제 완화.

    역전파시 일정 임계값을 넘어서면 그레디언트를 잘라냄.

 

11.2 사전훈련된 층 재사용하기

    비슷한 유형의 문제를 처리한 신경망을 찾아 전이학습 하는 것.

    전이 학습

        신경만의 하위층을 재사용

    1. 모델을 가져다 전부 재학습

    2. 모델을 가져다 하위 layer부분만 재학습

    3. 모델을 가져다 classifier 부분만 재학습



    비지도 학습훈련

        레이블된 데이터가 적거나 없을 때 사용

        비지도 사전훈련

            RBM, Auto Encoder, GAN 을 사용

 

11.3 고속 옵티마이저

    1. 모멘텀 최적화

        모멘텀 최적화가 경사 하강법보다 10배 빠르게 일어남.

        지난 그레디언트의 지수 감소 평균 사용

        단점

            하이퍼파라미터 하나 추가

    2. 네스테로프 가속 경사

        모멘텀 최적화의 변종.

        진동이 감소하고 수렴이 더 빠름

    3. AdaGrad

        적응적 학습률: 학습률을 감소시키지만 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소

        단점

            2차방정식에서는 잘 작동하지만 신경망 훈련시 너무 일찍 멈추는 경우가 종종있음. 심층 신경망에서 사용하면 X

    4. RMSProp

        위 단점을 최근 그레디언트만 누적하므로써 해결

        지난 그레디언트 제곱의 지수 감소된 평균

    5. Adam

        적응적 모멘트 추정. 모멘텀 최적화 + RMSProp

    6. AdaMax

        Adam 변종

        일반적으로 Adam이 더 성능이 좋지만 특정 조건에서 AdaMax가 좋음.

        Adam 보다 안정적

    7. Nadam

        Adam 변종. Adam + 네스테로프

        Adam 보다 빠른 수렴. 일반적으로 Adam보다 좋은 성능. 그러나 항상 그렇지만 않음.

학습률 스케줄링

    큰 학습률로 시작하여 학습속도가 느려질 때 학습률을 낮추는 방식.

    1. 거듭제곱 기반 스케줄링

    2. 지수 기반 스케줄링

    3. 구간별 고정 스케줄링

        적절한 학습률과 에포크를 찾기위해 다양한 시도 필요

    4. 성능 기반 스케줄링

        N 스텝마다 검증오차 측정 후 오차가 줄어들지 않으면 ㅅ 배만큼 학습률 감소

    5. 1사이클 스케줄링

        선형적으로 학습률을 올리다가 다시 내리는 방식.

        보통 더 좋은 성능을 가짐.

        단

            사용하기 어려움

 