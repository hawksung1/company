인공 신경망 소개에 앞서 DNN Model을 바닥부터 설계하기에는 엄청난 자원과 시간이 들어갑니다.
따라서 바닥부터 model을 만드는 것이 아닌 어느 정도 설계된 모델 즉 Pretrained model을 사용하여
model을 설계하는 것이 일반적입니다.
그러면 어떤 pretrained model을 가져다가 쓰는것이 좋을까 고민을 하게 되는데 이를 위한 사이트가 있습니다.

- SOTA
https://paperswithcode.com/sota

위의 사이트에서는 문제를 여러가지 카테고리로 나누어 분류하고 그에 맞는 모델이 benchmark 되어
순위로 나열되어 있는 사이트입니다.
따라서 실제 분석시에는 위 같은 사이트에서 pretrained model을 가져다가 사용하는게 좋습니다.
(모델 자체를 탐색할때는 automl을...)


인공신경망이란??
생물의 신경망에서 영감을 얻은 학습 알고리즘.
아래와 같이 기본적으로 입력에 대해 기본적인 계산을 거쳐 
특정값이면 활성함수가 활성화 되어 출력이 생기고 아니면 출력하지 않는 방식

하지만 이 단순한 모델은 xor 같은 논리문제를 해결하지 못하여 해결방책으로 나온 방식이 DNN 이다.

DNN의 학습은 3가지의 절차를 거치는데
정방향 계산 -> 역방향 계산 -> 경사 하강법 단계 의 절차를 거칩니다.
정방향 계산
입력값을 넣고 중간 계산값을 모두 저장하며 최종 결과의 출력오차를 계산

역방향 계산(역전파 알고리즘)
각 결과값이 최종출력오차에 기여하는 정도(오차 그레디언트)를 측정

경사 하강법 단계
경사 하강법과 오차 그레디언트를 이용하여 출력오차가 감소하도록 모든 네트워크의 연결 가중치를 수정
(SGD 알고리즘을 많이 이용, gradient descent를 사용하지 않는 방법도 존재
https://eng.uber.com/deep-neuroevolution/)


활성화 함수
1. Sigmoid
Gradient Vanishing Problem을 가지고 있어 DNN에서는 부적합

2. tanh
   sigmoid의 성능을 개선한 함수. 그러나 여전히 Gradient vanishing problem을 가지고 있다.


3. ReLU
tanh보다 속도도 빠르고 GVP도 발생하지 않는다.



SoftMax 함수
DNN으로 다중분류문제를 해결하기 위해 출력층에서 각 클래스의 예측확률을 구해주는 알고리즘

신경망 네트워크의 종류
1. fully connected(dense) neural network
모든 층에 모든 데이터를 학습, 이는 데이터에 있는 간단한 패턴이 연속된 변환으로 인해 왜곡될 수 있음.

2. wide & deep neural network
dense model 의 단점을 개선한 모델. 간단한 패턴, 복잡한 패턴 모두 학습 가능.


DNN 의 최적화된 하이퍼파라미터 찾기
grid search 보다는 random search를 이용한는 것이 좋다.
python package: sklearn model_selection RandomizedSearchCV
그러나 위의 방법은 너무나도 많은 자원과 시간이 소모 되므로 아래의 라이브러리를 활용하자
1. Hyperopt
2. Hyperas, kopt, Talos
3. Keras Tuner
4. skopt
5. Spearmint
6. Hyperband
7. Sklearn-Deap
8. 기타 대기업 알고리즘(구글 클라우드 ai 등)

중요 하이퍼파라미터
0. 은닉층과 뉴런 개수

1. 학습률
매우 낮은 학습률부터 매우 큰 합습률까지 점진적으로 수백번 반복하여 찾는다.

2. 옵티마이저
옵티마이저를 선택하고 옵티마이저의 하이퍼파라미터도 찾아야 한다.

3. 배치크기
보통 GPU RAM에 맞는 가장 큰 배치부터 줄여가며 찾는다.

큰 배치: 훈련시간을 단축
작은 배치: 일반화 성능이 좋음

4. 활성화 함수
일반적 ReLU 사용, 출력층은 문제 종류마다 다름

5. 반복 횟수
조기종료 옵션이 있어 보통 건들일 없음.
