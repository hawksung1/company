​11.4 ~ 12.3.4



11.4 규제를 사용해 과대적합 피하기



파라미터는 많을 수록 좋지만 그만큼 과대적합의 위험성이 커진다.

이를 위한 규제방법



1. l1, l2 규제



    - 케라스에서 사용법



    layer = keras.layers.Dense(100, actibation="elu", 

                                                      kernel_initializer="he_normal", 

                                                      kernel_regularizer = keras.regularizers.l2(0.01))

    - 방식
    


    l1: 많은 가중치가 0

    l2: 가중치를 제한



2. 드롭아웃



    -방식

    

    매 훈련 스탭에서 드롭확률 p 만큼 뉴런을 임시적으로 제외

    (다음 훈련 스탯에서는 초기화)



    - 주의점



    훈련이 끝난 뒤 각 입력의 연결 가중치에 본존 확률(1-p) 의 값을 곱해야함.

    수렴을 느리게 함

    모델 평가시에는 제외해 줘야함.

    SELU 활성화 함수에는 alpha drop out을 사용해야 함.

    

    - keras



    일부를 랜덤하게 출력 0 으로 설정

    입력값을 보존 확률로 나눔.

    keras.layers.Dropout(rate=0.2)







    - MC dropout(Monte Cario/몬테 카를로)

    

        훈련한 dropout model을 재훈련 하지않고 성능을 향상시킴



        - 방식



            model test 할때도 드랍아웃을 사용하여, 여러번 예측한후 평균을 낸 값을 사용한다.



        - 참조



             https://datascience.stackexchange.com/questions/44065/what-is-monte-carlo-dropout





3. 맥스-노름(max-norm)



- 방식







- 케라스

keras.layers.Dense(100, activation="elu", kernael_initializer="he_normal", kernel_constraint=keras.constraints.max_norm(1.))



- 참조

https://honeyjamtech.tistory.com/tag/MC%20dropout





4. 가이드라인



- 기본 DNN

 

 하이퍼파라미터

 기본값

커널 초기화 

He 초기화 

활성화 함수 

ELU 

정규화 

배치 정규화(깊은 신경망)

규제 

조기 종료 or l2 규제

옵티마이저 

모멘텀 최적화(RMSProp or Nadam) 

학습률 스케줄 

1사이클 

 

- 자기 정규화 DNN(완전 연결, 단순한 모델인 경우)



 하이퍼파라미터

 기본값

커널 초기화 

르쿤 초기화

활성화 함수 

SELU 

정규화 

x

규제 

알파 드랍아웃

옵티마이저 

모멘텀 최적화(RMSProp or Nadam) 

학습률 스케줄 

1사이클 



- 그외



    입력 정규화

    사전 모델 조사

    희소 모델이 필요한 경우: l1 규제, 텐서플로 모델 최적화 툴킷(TF-MOT)

    빠른 응답이 필요한 경우: 층 개수 줄임, 배치 정규화 합침, LeakyReLU 사용, 소수점 정밀도 낮추기. TF-MOT

    정확도가 중요한 모델: MC dropout



- 참조



https://www.tensorflow.org/model_optimization?hl=ko

    



12.1 텐서플로 훑어보기



- 제공 기능



    GPU, TPU

    분산 컴퓨팅

    Just In Time 컴파일러

    A 환경에서 프로그래밍 하고 B 환경에서 실행 가능

    자동 미분, 고성능 옵티마이저 지원



- pre trained model site



    https://www.tensorflow.org/hub?hl=ko

    https://paperswithcode.com/



- tensor 란 무엇인가??



    array 라고 생각하면 편함. 그러나 스칼라값도 될 수 있음(숫자 하나)

    따라서 numpy ndarray 처럼 다룰 수 있다.



- 텐서와 연산



    - 사용법



        - Init



        tf.constant([[1., 2., 3.], [4., 5., 6.,]])
****
        tf.constant(43)



        - Use



        t + 10

        tf.square(t)



    - 특징



        모든 tensor 는 변환 불가(위 처럼 변환식을 사용하는 경우 새로운 tensor가 만들어짐)



    - tf.Variable



        tensor의 파라미터를 변경할 때 사용

        keras 에서는 tensor 계산시 알아서 variable 로 변경해줌



    - 기타



        - 사용자 정의 loss function 사용가능

        - 사용자 정의 요소 모델을 Save, Load 하는 기능

        - 활성화 함수, 초기화, 규제, 제한 등 대부분의 함수 커스터마이징 가능.



