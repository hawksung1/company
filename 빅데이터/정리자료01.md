## 빅데이터분석 기출 기반 정리자료01

### 단답형

1. VC Dimension / Hoeffding's Inequality

   vc dimension: 통계적 분류 알고리즘으로 학습 할 수 있는 함수 공간의 용량 -> 모델의 복잡도 라고 이해

   Hoeffding's Inequality(호에프딩 부등식) : 경계(boundary) 의 독립 확률 변수의 합이 예상 값에서 일정량 이상 벗어날 가능성에 대한 상한 값을 제공

    in-sample error와 out-sample error로 내가 가지고 있는 분류기가 있을 때, 학습에 사용된 데이터에 적용시켰을 때 결과와 새로운 데이터에 적용시켰을 때의 결과 차이의 평균을 "확률적"으로 bound 시키는 것

   https://enginius.tistory.com/465

   http://theyearlyprophet.com/learning-from-data-learning-theory-kr.pdf

2. word2vec 

   중심단어를 사용하여 주변단어를 예측하는 skip gram 방식과, 주변단어로 중심단어를 예측하는 CBOW 방식이 존재함. 

   https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/

3. Glove

   https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/

4. Q-Q plot

   수집 데이터를 표준정규분포의 분위수와 비교하여 그리는 그래프로, 수집 데이터가 정규성 가정을 만족할시 y = x 형태의 직선 모양으로 그려짐.

5. 시계열 분석 검증법

   시계열 분석..

   - **Arima 모형** (승법계절 아리마 p,d,q 모형) 

     데이터가 정상 시계열인지 확인 

     -> 해당 시계열 데이터가 시간에 독립인지 / 시계열의 분산이 일정한지 / 시계열의 평균 M 이 시간에 대해 constant 한지 확인 (다 같은말임)

     정상 시계열이 아니라면  (데이터에 추세가 보인다면) 적절하게 차분을 시행하여 시계열을 정상시계열로 변환해주는데, 비계절 차분과 계절차분으로 나뉘며 계절성이 뚜렷하게 보이는 비정상 시계열에는 계절 차분을 시행하고, 그렇지않다면 비계절 차분을 시행함.

     **이후 분석하는 시계열 데이터는 정상성 가정을 만족하며, 잔차는 자가상관을 가지고 있지 않은 백색잡음이라고 가정함**

   - 검정방법

     1. 단위근 검정 -> 차분을 진행 할지 말지 결정 

        ∇Yt=aYt−1+ϕ1∇Yt−1+ϕ2∇Yt−2+⋯+ϕp∇Yt−p+et  ( yt의 변화량을 종속변수, 원 시계열과 차분값의 지연 값들을 독립변수로 회귀분석을 시행하여 구한 계수값 a)

        H0: 데이터가 정상 시계열을 만족한다 (a = 0)

        H1: 데이터가 정상 시계열을 만족하지 않는다.(a !=0) 

        피벨류가 유의확률보다 작으면(검정통계량 값이 유의수준의 Z값보다 크면) 귀무가설 기각 -> 데이터가 정상성 가정을 만족하지 않으므로 데이터 차분을 진행하여야 함.

     2. 포트만토 검정 -> 잔차의 자가상관을 확인함

        H0: 잔차가 자가상관이 없음(p1=p2...=0)

        H1: 잔차에 자가상관이 존재함 

        피벨류가 유의확률보다 커 귀무가설이 채택되면 -> 잔차가 자가상관이 없으므로 모형이 제대로 만들어진 것.

        피벨류가 유의확률보다 작아 귀무가설이 기각되면 -> 잔차에 자가상관이 남아있는 것이므로 모형 수정이 필요함.

        

6. 가설검정/기초통계

   - 가설검정

     -> 시계열 검정 부분에서 먼저 나왔는데, 통계의 기본 가설검정은 귀무가설, 대립가설 세우는것부터 시작

     귀무가설 : m =0 이다의 형태 (독립이다/ 계수가 유의미하지않다 )

     대립가설 : m>0 or m<0 (단측검정) , m != 0 (양측검정)

     단측검정의 경우 검정통계량 값 > Z값 (m>0) 검정통계량 값 < Z값( m<0 ) 이면 귀무가설 기각, 양측검정은 검정통계량 > |Z/2| 면 귀무가설 기각. 

   - 기초통계

     https://blog.naver.com/PostView.nhn?blogId=mykepzzang&logNo=220842759639&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView

     이항분포(binomial): 확률 p 인 사건을 N번 시행하여 사건 발행 횟수에 따른 확률들을 구하면 그것을 확률 p, 시행횟수 N인 이항분포라고 정의하며 B(N,p)로 표현함. 평균은 Np,이며 분산은 Np*(1-p) 임 중심극한정리에 따라 N 이 커지면서 정규분포로 근사함

     포아송분포(poisson): 단위시간 안에 어떤 사건이 몇 번 발생할 것인지 표현하는 이산 확률 분포로, 일어나는 사건이 독립적임.  평균과 분산 모두 모수 람다 값이며 np의 값이 5보다 작으면 이항분포는 람다= np 인 포아송 분포에 근사하게 됨 (주로 count 데이터에 활용함)

     초기하분포(hypergeometric): 크기가 N인 유한한 모집단으로부터 n개를 비복원추출 할 시에 확률변수 X가 나타내는 이산 확률분포로, 평균은 nk/N이며 분산은 (N-n/N-1)n(k/N)*(1-k/N) 임

     기하분포(geometric): 성공일 확률이 p 인 베르누이 시행을 독립적으로 반복하고 처음 성공할 때까지의 시행횟수를 확률변수 X라 할때, X가 나타내는 이산확률분포. 평균은 1/p 이며 분산은 1-p/p^2 . 음이항 분포에서 k =1 인 특수한 경우임.

     감마분포(Gamma): 알파 번째 사건이 일어 날 때까지 걸리는 시간에 대한 연속 확률분포, 평균은 알파와 베타의 곱이며 변수는 알파와 베타 제곱의 곱임

     지수분포(exponential): 사건이 서로 독립적일때, 일정시간 동안 발생하는 사건의 횟수가 포아송 분포를 따른다면 다음 사건이 일어날 때까지의 시간에 대한 분포는 지수분포를 따름 감마분포에서 알파값을 1로 고정시킨 특수한 경우이며 평균은 모수 베타값이며 분산은 베타제곱임

     음이항분포(Negative Binomial): 성공확률이 p 인 베르누이 시행을 독립적으로 반복하고, k번째 성공이 나올때까지의 시행횟수를 확률변수 X라 할때, X가 나타내는 이산확률분포 평균은 k/p, 분산은 k(1-p)/p^2 

     카이제곱분포(chi-square): 감마분포에서 알파가 v/2 베타가 2에 해당하는 특수한 경우로 자유도 k 에 따라 분포가 달라짐. 연속확률변수 X가 평균이 m 이고 분산이 sigma^2 인 정규분포를 따를때, (x-m)^2/sigma^2 은 자유도 1인 카이제곱 분포를 따름.

     T분포: 모집단의 분산이 알려져 있지 않은 경우에 정규분포 대신 이용하는 확률분포로, 평균이 m이고 분산이 sigma^2인 정규분포에서 추출한 표본 x1,x2..xn에 대한 표본분산을 S^2라고 할때 T = mean(x)-m/(S/n^0.5) 는 자유도가 n-1 인 티분포를 따름

     F분포:정규분포를 이루는 모집단에서 독립적으로 추출한 표본들의 분산 비가 나타내는 연속 확률 분포로, 2개 이상의 표본평균들이 동일한 모평균을 가진 집단에서 추출되었는지 아닌지를 판단하기 위해 이용함. 서로 독립인 확률변수 U,V가 각각 자유도가 u,v인 카이제곱분포를 따를때 새 확률변수 F = (U/u)/(V/v) 는 자유도가 (u,v) 인 F분포를 따름

## 알고리즘 이론정리

### 회귀분석

회귀분석에서, 회귀계수값을 제대로 추정하고 이를 신뢰하기 위해서는 필요한 몇가지의 가정들이 있음. 

1. <mark>모델의 형태에 대한 가정</mark>

   Y는 X와 파라미터들의 선형 결합으로 이루어진 형태이다.

2. <mark>에러(잔차)에 대한 가정

   - 에러 e1,e2..en이 iid(identically and independently) 하게 평균이 0이고 분산이 sigma^2인 정규분포를 따름

     -> <mark>에러의 정규성</mark>: 에러는 정규분포를 따라야 함

     -> <mark>에러의 등분산성</mark>: 에러는 constant 하여야 함 (증가하거나 감소하는 trend 가 존재해서는 안됨)

     -> <mark>에러의 자기상관 부재</mark> 에러들끼리는 서로 독립이어야 함

     -> 이때 해당 가정에서 정규성 가정을 만족하지 않아도 회귀계수 추정값은 unbiased estimator임.. 이때 비교대상을 선형이면서 Unbiased 추정량들의 집합으로 한정할 경우에, Least square estimator 는 불편추정량이자 최소 분산 (smallest variace) 을 지닌 최량선형불편추정량(Best Linear Unbiased Estimator)임 (가우스-마코브정리)

     불편추정량 - 편향되지 않은 추정량

     <mark>LSE(Least Square Estimator)</mark> -최제추정량이란?

     - 회귀계수 추정방법으로, 최소제곱법을 이용하여 잔차의 제곱합이 최소가 되도록 함. 

3. <mark>predictors 에 대한 가정

   - 설명변수들은(predictor variables) 모두 non-random이다

     -> 따라서 반응변수의 정규성은 에러의 정규성에서 기인함

   - 설명변수의 관측값 xij에는 error 가 존재하지 않는다

   - 설명변수 x1,x2,,,xp는 서로 독립이다.

**다중공산성(Multicollinearity)**

- **독립변수가 다른 독립변수들의 선형 결합으로 나타낼 수 있을때**(즉, 독립변수들 사이에 서로 강한 연관관계가 존재할때) 발생하는 문제로, 이 경우 X tranpose x X 행렬의 역행렬이 존재하지 않는 문제가 발생하며 **회귀계수 추정값을 LSE로 계산할수 없는 문제가 발생**함.

  -> 다중공산성 문제가 발생할 경우, 개개 변수들이 반응변수에 미치는 Unique 한 영향도를 측정하는것이 불가능해 지며, 예측된 회귀계수값이 데이터의 변화에 매우 민감하게 변하는 문제가 발생함. 즉 **모델 일반화가 힘들어짐**.

  즉 예측한 회귀계수의 sampling error 값이 매우 커지기 때문에 해석과 예측 두가지 경우가 모두 힘들어짐

- 다중공산성 문제는 모델링 에러는 아니고, 대체로는 <mark>데이터의 부족 문제</mark>로 인해 발생함

- 다중공산성은 Variance Inflation Factor, **VIF** 를 통해 파악 할 수 있으나 모든 다중공산성을 표현해 주지는 않음

  -> VIF 는 1/1-R^2로 표현되며, 통상적으로 vif가 10 이상일때 변수에 다중공산성이 존재한다고 파악하나 10이라는 숫자에 통계이론적인 근거는 찾기 힘듬

추가적으로, 회귀분석의 계수에 대해 해석할때는 "when other factors are constant" 라는 가정이 필요함. <mark>다른 변수가 고정적일때를 가정하지 않고는 해당 변수가 회귀계수만큼 반응변수에 영향을 미친다고 해석 할 수 없음</mark>



### Generalized Linear Model(GLM)

numeric 변수만을 반응변수로 하는 회귀분석의 일반화된 모형으로, link function 의 종류에 따라 다양한 모델이 존재함.

GLM 은 3가지 부분으로 구성되어 있는데, random component 인 반응변수 Y, systematic component 인 설명변수 X, 그리고 Y와 X사이의 관계를 규명하는 link function임. 근데 여기서 link function 이 되기 위한 조건이 두가지 존재하는데,바로 <mark>monotone(단조증가)</mark> 하고 <mark>differentiable(미분가능)</mark> 하다는 점임. 이 두가지 조건을 만족하지 않은 함수는 link function 이 될 수 없음.

1. Poisson regression

   반응변수가 count data 일때 주로 사용함, 반응변수의 평균과 분산이 모두 모두 m(모수) 로 일정하며 link function g(m) = log(m) 임 

   실제로 데이터의 분산(m)이 평균값(m) 보다 크게 될 때 포아송 회귀를 사용하게 되면 Overdispersion 문제가 발생하게 되며, 이는 데이터들의 이산성으로 인해 쉽게 발생하는 문제임

   -> 이 경우에는 Y의 평균과 분산값이 다르지만 동일한 link function 을 사용하는 Negative Binomial 회귀를 사용하면 해결됨.  NB 회귀는 Y가 NB(m,k) 를 따른다고 가정하며, 이때 분산 k는 m + Dm^2으로 표현됨. D가 0으로 수렴함에 따라 overdispersion 문제가 발생하지 않는다고도 볼 수 있음. 

2. Logistic regression

   반응변수가 Binary 일때 사용하는 회귀로, link function 으로 sigmoid 를 사용함. logit(p) = log(p/1-p) = a+bx. p는 성공확률

   다른 변수들이 constant 할 때, x가 1단위 증가할때 마다 오즈가 exp(beta) 배 증가한다고 해석. 베타=0 이면 x가 변화해도 오즈 변화 없음.

3. Multicategory Logit Model

   반응변수가 binary 이상의 범주를 가지고 있는 변수일때 사용하며 nominal일 경우 baseline category logit을, ordinal 의 경우 cumulative logit 을 사용함

   - baseline category logit

     -> Y1,Y2..Yn이 iid 하게 multinomial(1,p1,p2..pJ) 을 따른다고 가정함. J는 카테고리의 갯수. 마지막 카테고리 J가 baseline이 된다고 할 때, baseline category logit 은 log(pj/pJ), j = 1,2,...J-1 로 표현 가능.

     즉, log(pj/pJ) = aj + Bjx 이며 J=2 일땐 로지스틱 회귀 모형과 동일해짐. 

     베이스라인 카테고리 로짓 모형은 카테고리를 짝지어서 비교하는 형태로 총 J-1개의 식이 동시에 피팅되게 되며 각각의 피팅에 비교하여 SE가 줄어듬.  

   - Cumulative logit 

     -> 반응변수의 카테고리가 ordered 된 경우에 로짓에 순서 정보를 반영할수 있고, 베이스라인 로짓 모델에 비해 해석이 간단하며 성능이 좋음.

     logit(P(Y<=j)) = log(P(Y<=j)/1-P(Y<=j)) = log[(p1+p2..pj) / (p1+p2...pJ)] = aj + Bx

     베이스라인 모형과는 다르게 변수 x가 반응변수에 미치는 영향이 J-1개 모두 B로 동일함. 즉 추정되는 베타 계수가 한개인데 이를 적용하기 위해서는                                                        각각의 cumulative 확률에 같은 확률 베타가 적용된다는 proportional odds 가정이 필요함(확률 오즈 모델과 j번째 카테[고리에 대한 분리 모형 Bj와 비교함)

GLM 모델들을 서로 비교할때는 Deviance 를 이용함. 

H0: 현재모델 유지 H1: full model

Lm, Ls를 각각의 모델에 대한 maximized log-likelihood라고 할때, GLM의 deviance 는 -2(Lm-Ls) 로 표현 가능하며 이를 LR statistic 이라고 함. 해당 통계량은 자유도가 두 모델의 파라미터 갯수 차이인 카이제곱 분포에 근사하며, 값이 클수록 귀무가설을 기각. 



### 나이브베이즈 분류기(Naive Bayesian Classifier)

스팸메일 필터, 텍스트 분류, 감정분석, 추천시스템 등에 활용되는 분류 기법으로 . 특정 개체 x가 특정그룹 c에 속할 확률 즉 p(c|x) 이하 사후확률을 구하는 것이 목적임. p(x|c) 는 특정그룹 c인 경우에 특정 개체 x가 그룹에 속할 조건부 확률이며, likelihood임. p(c)는 특정 그룹 c가 발생할 빈도, 즉 클래스 사전 고유 확률(prior probability)라고 함.

**특징**
  1. 지도학습
  2. feature에 따라 라벨을 분류함에 있어 베이즈 정리 사용

**가정**
  1. 모든 feature가 서로 독립

**목적**
  1. x 가 특정그룹 c에 속할 확률, p(c|x)

사후확률 ~ Likelihood x 사전확률 로 요약표현가능

나이브베이지안은 

- multi-class 분류에서 쉽고 빠르게 예측이 가능함

- 설명변수들이 모두 독립이라는 가정이 만족한다면, 여타 다른 분류기들(로지스틱,,) 에 비해 훨씬 성능이 좋으며 학습데이터도 적게 필요함.

- 수치형(numeric) 데이터들보단 범주형(categorical) 데이터에 특히 효과적임.

- computation cost가 적기 때문에 매우 빠름.

- train 데이터에는 존재하지 않고 test 데이터에만 존재하는 범주에 대해서는 확률이 0이 되기 때문에 정상적인 예측이 불가능함.

  -> 이것을 zero frequency 라 부르며, 이를 해결하기 위해선 라플라스 추정을 이용하여 smoothing 을 하여야함.

- 설명변수 독립 가정이 성립하지 않을시에는 에러 발생 가능성이 매우 높아져서 결과를 신뢰할 수 없음.     

   

   

   

   
