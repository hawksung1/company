# **2020**

24문항, 200분

| 분류 | 소분류 | number | Score |
| --- | --- | --- | --- |
| 분석시나리오 정의 | ㅇ | 3 | 24 |
| 분석 방법론 구체화 | ㅇ | 3 | 24 |
| 데이터 전처리, 마트구성 | 데이터 전처리파생변수 생성분석 마트 구성 | 3 | 24 |
| 데이터 탐색 | 시각화분포이해 | 2 | 5 |
| 기초 통계 분석 | 추정, 검정샘플링기본 개념다변량 | 3 | 7 |
| 모델 이해 | ML / DLText 분석 | 5 | 19 |
|
 | 모델 진단, 평가모델 적용, 활용 | 2 | 5
 |
| 데이터 분석 cloud | AWS 환경 data 탐색기초 통계모델링Data Bricks 활용 | 3 | 8 |

Cloud: AWS 서비스 중 분석

DAP MLDL 에서 서비스

AutoML

AWS Technical Essential 강의, 클라우드 환경에서 쓰는 분석기능(PySpark 등)

서술형
  1. Cloud에서 분석 방법론 구체화 문항(Cloud 환경 특이점(탐색, 모델링 관점)을 명시해 서술
2. 데이터 엔지니어링(교재 참조)

# **2019** 년 기출 객관식 키워드

트리에 기반한 모델에서 얻어지는 예측 값은 학습데이터에 존재하는 관측 값의 범위 안에만 있음.

**Bayesian Posterior interval** = 베이즈 신용 구간

    신용 구간이 사전 분포와 문맥 정보 혼합
    Prior 분포(사전 분포): 미리 알고 있던 모수의 분포
    Margin of error(허용 오차):

**빈도주의 신뢰 구간**

    신용 구간이 데이터에만 의존

### SVM

    분류를 위한 선(면)을 정의(마진이 최대화 되는)
    Non support vector: svm에 사용되지 않은 vector(있던 없던 상관 없음)

Bias, variance trade-off: 오버피팅-bias 감소, variance 증가

### **Poisson Regression** 
    
    평균과 분산이 같아야 함. 단위사간에 사건이 몇번

Gaussian 모델(정규 분포):

### 이산 확률 분포

    Binomial 모델(이항 분포): 1번 실행시 베르누이 분포, 성공/실패
    Negative Binomial 모델(음 이항 분포): k번 성공 확률 분포
    Multinomial: 주사위
    Geometric: 베르누이 시행을 성공할때까지의 확률변수의 분포

Boosting: 가중치

Bagging: 투표

### 로지스틱 회귀 추정
    likelihood 최대화, MLE(Maximum Likelihood Estimation)
    likelihood: 독립 사건 = 확률, 연속 사건 = PDF 값

### Ordinal logistic regression(카테고리에 순서가 있을 때 사용)

    Proportional odds model: log odds와 독립변수 사이의 선형성을 가정,
        Proportional odds assumption: 1,2,3 이 있을 때 1,2 거리, 2,3의 거리가 같다.

Multinomial logistic regression: log odds와 독립변수 사이의 선형성을 가정

### 카이제곱 검정

    기본 가정: n(빈도) \&gt;= 5, 관측치 서로 독립.

### **SGD**(Stochastic Gradient Decent): mini batch에 대해 loss function을 계산, batch size 등가시 BGD랑 차이가 적어짐.

    장: local minima에 빠지지 않음
        빠름

    단: 부정확

BGD(Batch Gradient Decent): Batch 전체에 대해 loss function 계산

Vanishing gradient: 레이어가 적으면 발생했다고 할 수 없다.

OLS(Ordinary Least Squares): 결정론적 선형 회귀. RSS 최소화.

    Blue(Best Linear Unbiased Estimator): Unbiased, 가정 효율적 추정방법.

        1. 선형성
        2. 오차항 평균 0
        3. 동분산
        4. 오차항 독립
        5. 독립변수가 주어진 것이라는 가정

GLS(Generalized Least Squares)

Naive bayes: 결측치를 처리하지 않고도 사용 가능

#  **2019** 년 기출 서술형 키워드

A+B = C, 총 4시간, 5분마다 데이터 측정, 하루에 4번, 배치마다 온도, 압력, 유량등을 조절

변수 개수 \* 시간 개수 > 배치 수 = PLS else 뉴럴, 랜포 등

### PLS(부분 최소 제곱)

    1. 예측 변수가 매우 공선적이거나 예측 변수의 수가 관측치의 수보다 많으며 범용 최소 제곱법에서 계수를 생성하지 못하거나 표준 오차가 높은 계수를 생성할 경우에 특히 유용
    2. 다중 회귀 분석과 달리 예측 변수가 고정되어 있다고 가정하지 않음
    3. 즉, 예측 변수를 측정할 때 오류가 발생할 수 있지만 불확실성을 측정하는 데 더 로버스트

서술형 서술 순서

  1. 데이터 자체의 분석(독립성 검정, 종속변수 설정 등)
  2. 모델의 선정(지도, 비지도 등), 그 이유
  3. 데이터 전처리(na 값 처리, 표준화, 영향력 파악)
  4. 모델 학습 방법
  5. 모델 검정(cross validation 등)
  6. 예측

배치가 다 돌지 않은 상태에서의 예측

    지금 데이터를 기반으로 배치가 끝났을 때의 데이터 값을 예측하여 그 데이터를 기반으로 예측

# **2018**  객관식 기출 키워드

SARIMA 모형: Seasonal ARIMA

Stationary time series(안정시계열) 데이터에 적ㅇㅇ 가능하므로 안정성을 만족

    1. 평균이 일정할 것
    2. 각 시점별 분산이 동일항 것
    3. 시점간 공분산이 시차에만 의존할 것

Naïve Bayes: X 독립을 가정, 결합확률분포가 각 X변수들의 확률 곱으로 표현

포아송: 단위시간당 발생한 사건 빈도, 평균과 분산이 같아야 함. 동일하지 않을경우 음이항 분포를 가정

**GLM 모델: Generalize Linear Model**

  독립변수와 종속변수 사이의 선형성,
  오차항의 정규성
  독립성
  등분산성
  4가지를 만족해야 하는 기본적인 회귀모델과는 다르게
  오차의 정규성, 등분산성을 만족하지 않아도 됨

    종속변수의 분포가 정규분포인 경우: gaussian
    이할분포인 경우: binomial
    포아송 분포인 경우: poisson
    역정규 분포인 경우: inverse gaussian

회귀분석, 분산분석: 종속변수가 정규분포되어 있는 연속형 변수

음이항 분포: k번 성공할때 까지 시행

부트스트랩 :

    원래의 모집단이 iid 가정을 충족
    재표집의 수가 적을 경우 아웃라이어의 영향 받을 수 있음
    분석 할때마다 결과가 조금씩 다름(대표집 수를 5000회 이상시 무시)

    Idd
        Independent and identically distributed
        확률변수가 여럿 있을 때 서로 상호독립, 동일한 확률분포
    중심극한 정리에 의해 parameter 추정치들이 평균은 참값으로 수렴
    모집단의 분산이 큰 경우 부표본의 반복추출 횟수, 크기를 늘려야 함
    부표본 parameter 추정치들은 정규분포를 따름

Content based filtering: 단순 연관 추천.

Collaborative filtering: 행동양식 기반, 다른 유저와 비교하여 추천

Pearson 상관계수: 교호작용을 고려하지 않음

    교호작용: 한 요인의 효과가 다른 요인의 수준에 의존하는 경우

PCR(Principal Component Regression)

    독립변수만을 고려하여 주성분을 추출

PLSR(Partial Least Square Regression):

    원본 데이터 대신 상관 관계가 없는 성분 집합으로 예측변수를 줄이고 이러한성분에서 최소 제곱법을 수행하는 방법.
    독립변수와 종속변수의 상관관계를 고려하여 주성분을 추출
    단: 예측 변수가 고정되어 있다고 가정하지 않기 때문에 오류 발생

F통계량: 모형이 통계적으로 유의미 한가 체크

T통계량, p val, 신뢰구간 확인: 회귀 계수들이 유의미 한가

결정계수: 모형이 얼마나 설명력을 지니는가

VIF, 다중공산성 여부

Normal QQ plot: 정규성 가정에 대한 검토

Epoch = all data

All data /batch size = n iteration

# **2018**  **서술형 기출 키워드**

예측방범론 -> 모델 설계 바닥부터(변수 설명, 모델 선정 등등)

=========================================================================
# 데이터 엔지니어링

### ETL

1. Extract, Transformation, load - 데이터를 추출 변환하여 타겟시스템에 적재
2. 다양한 형식의데이터를 한 시스템에 통일된 양식으로 모으는 작업
3. 데이터 이동과 관련된 모든 분야에 활용

    1. BI 및 데이터 분석을 위한 수집
    2. 마스터 데이터 관리
    3. 데이터 이관 + 빅데이터 이행과제 지원
    4. 같은 데이터를 사용하는 두 시스템의 데이터 동기화
    5. 기업 간 데이터 전송
    6. 데이터 서비스
4. 갖추어야 할 것

    1. 설계 개발이 되고, 프로그램을 실행한 것에 대한 모니터링 가능
    2. 다양한 시스템에 접근이 가능해야 함

5. ETL 툴 선정 기준

    1. 호환성
    2. 여러 테이블을 join, split 가능해햐
    3. GUI 직관, 오류처리, 보안, 멀티(여러개 동시)
    4. 배치 스케줄링, 타겟 컬럼 자동매칭, 자업 소요시간 표시
### 매핑 작업 순서
  1. 소스 시스템 분석
    
    1. 소스 시스템 내 테이블 분석
    2. 위에서 분석한 테이블 컬럼 분석
  2. 타겟 시스템 분석

    1. 데이터 넣을 DB구현
    2. 소스 시스템 분석을 초대로 타겟의 테이블, 컬럼 정의
  3. 매핑 정의
  4. 어떤 테이블의 어떤 컬럼과 연결시킬지 결정
  5. 추출하는 횟수 설계
  6. 데이터를 변경시키는 로직도 설계
  7. 이 모든것을 매핑 정의서로 작성

### 추출 및 전송

  1. 추출 전송 방식
    1. 온라인: API나 쿼리문으로 받아오기
      1. 일반적인 방식
      2. 하지만 운영서버가 해당 데이터를 보내주기위한 작업을 너무 오래하면 안됨
      3. 따라서 소량 전송시 주로 사용
    2. 오프라인: 파일로 받아오기
      1. 초기 적재시 사용, 운영서버에 무리가 적다
  2. (운영중)추출 방식
    1. Refresh: 싹 갈아엎기
      1. 데이터가 적은 디멘젼 등에 적용 - 디멘젼(dimension): 비즈니스 이벤트에 독립적인 데이터
    2. Timestamp: 시간 비교
      1. 데이터 변경일자를 확인하여 마지막 추출 일자보다 나중 것만 추출
      2. 원래 소스 시스템에 변경일자 컬럼이 필요
    3. Snapshot 비교: 데이터 적재된 것을 비교
      1. 담겨있는 전체 데이터를 캡쳐하여 전후를 비교
    4. DBMS 로그: 변경 내역 관리
      1. 로그에 변경 내역만 올려둠
      2. 로그를 확인하여 변경내역만 추출
    5. Application log: 변경 내역 관리
      1. 로그 파일말고 어플리케이션에 기록
      2. 파일 저장이 아니라 어플리케이션에 부하가 크고 복잡

### 변환

  1. 데이터 표기 방식 통일
    1. Y/N, 0/1, T/F를 로직을 통해 하나로 변경
    2. 시간 표기 방식 통일
    3. NULL 값을 처리하는 방식
    4. Key값 통일
    5. Comment
    6. 집계함수(Min, Avg, sum 등)을 사용
    7. 파생변수(계산 및 로직이 필요한 컬럼)

### 적재

  1. Append
  2. Delete + append
  3. Truncate: 기존 데이터 싹다 날림
  4. Insert: append를 DB에서 함
  5. Create: 임시 테이블을 생성하여 적재(툴내 자동 적용)
  6. 적재할 때는 적재 시간과 용량을 고려

### 검증

  1. Data를 제대로 못 불러와 공백,null 등이 생겼는지
  2. 최소 최댓값
  3. 건수
  4. 코드 데이터가 제대로 들어갔는지
  5. 날짜 데이터 확인
  6. 업무 로직 검증

### 배치

  1. 고려해야 할 사항
    1. 배치 시각
    2. 데이터간 선후행 관계
    3. 동시에 가능한 개수
    4. 배치 그룹핑
  2. 담당자가 없어도 상관 없어야 함
    1. 오류가 발생하면 영향있는 후속작업이 자동으로 정지
    2. 후속 작업은 선행작업 바로 다음에 연결 OR 끝나는 시간 예측하여 진행

### 정제

  1. 정제는 되도록 소스 데이터를 수정
  2. 변환을 사용하면 로직이 복잡해지고 후에 소스 수정시 문제가 자주 발생
  3. 바꿀 수 없다면 ETL에서 진행 ?????
  4. 품질 관리 프로세스
    1. 데이터에 문제가 ㅇ벗는가
    2. 약간 있는 문제가 큰 영향이 있는가
    3. 데이터의 담당부서, 필요없는 부분, 어떠한 데이터, 데이터 변경 비용, 사용처
    4. 바꿀 방법
    5. 바꾸고 난 뒤 영향도에 따라 후속 작업 진행
    6. 다시 품질 확인
  5. 정치적인 방식
    1. 운영계 시스템에서 데이터에 대해 품질을 검증하고 개선
    2. 데이터가 이상함을 현업에게 고의로 노출
    3. 최대한 소스쪽 수정을 요구, 부득이한 경우만 ETL에서 수정
    4.

# **MLDL**

### SBP
  데이터 저장/처리 플랫폼
  주요기능

    1. 대용량 수집 & 전처리
      1. apache NiFi 적용
      2. 웹 기반 GUI, workfloe 작성, 제어, 피드백, 모니터링 편의성
    2. 데이터 저장
      1. 로컬 스토리지: HDFS
        1. 분산 파일 시스템
        2. 처리 관리 모니터링
      2. 메시지 큐: Kafka
        1. 실시간 데이터 스츠림 전처리
      3. 클라우드 스토리지: AWS S3, MS Azure, Google CloudStorage
        1. HDFS에 비해 확장성, 관리 편의성 UP BUT 성능 DOWN
        2. 잔순 데이터 저장에 주로 사용   
    3. 데이터 처리
      1. SQL 기반 처리
      2. In Memory 기발(Spark + Yarn)
      3. 실시간 Stram 처리
      4. NoSQL 처리(Hbase, Phoenix)

**NoSQL 적합 요건**

  1. 대규모 정형, 반정형 데이터에 대한 효율적인 random read/write 수행
  2. 수십 TB에 대한 초당 수천만 건의 처리 수행 
  3. 조회 패턴이 잘 알려져 있고 단순한 경우 
  4. 대규모 데이터 저장이 필요하고, 데이터 증가에 따라 확장이 필요한 경우

![platform_compare](./img/platform_compare.png)      


# **AI**

넓은 의미의 ai: 자동화 프로그램 모두

좁은 의미의 ai: 인간의 사고 행동 표방

딥 러닝은 머신러닝에 포함됨, 머신러닝은 넓은의미의 ai에 포함

Knowledge based ai: 사람이 자신의 인사이트로 end to end rule설정

ML based ai: data를 기반으로 인식, 예측 rule

Representation learning: data based feature creation

Deep learning based ai: neural network, all auto

정형 데이터: 엑셀 등

비정형 데이터: 이미지, 영상, 소리 등

### Ai 구현 process

  1. 전문가 시스템
    1. 모든 rule을 사람이 결정
  2. 전통적인 ml 기법
    1. Feature를 사람이 결정하여 rule은 모델링
  3. Ml/dl 기법
    1. Raw data 부터 rule 까지 모델링

Tensor: scalar -\&gt; vector -\&gt; matrix -\&gt; tensor(3d)

Fnn: feedforward neural network

Cnn: convolutional neural network

Rnn: recurrent neural network

Sigmoid: gdp 발생, 계산 복잡

Tanh: gdp 발생

Relu: 굳

Gradient descent

Back propagation

Hyperparameter: traning 동안 학습되지 않음

Learning rate

Data: 4000, batch size=400 ---\&gt;\&gt; 1epoch = 10 iteration

검출율: tp/(tp+fn) --\&gt; 불량 검수 빡세게

정밀도: tp/(tp+fp) --\&gt; 불량 검수 자체를 정제


### Loss function

Quadratic

장: 실수에도 사용 가능

단: sigmoid와 사용시 수렴이 느림

Cross entropy

이진 분류에서 사용

장: quadratic에 비해 모델 수렴

Negative log likelibood

Multiclass 에서 사용

장: softmax와 사용ㅅ 수렴이 빠름


### Optimizer

Momentum: 과거의 파라미터 업데이트 내역을 누적

Adaptive: 파라미터 업데이트가 진행될수록 learning rate를 줄임


### 가중치 초기화(weight initialization)

Lecun normal: relu 나오기 전, 가우시안 분포에서 분산을 x의 원래 분산 정도로 보정

Xavier: relu 이후, 입력/출력 노드수를 고려하여 초기값 설정

He: xavier 분산값을 2로 곱함


### Regularization

Generalization error를 감소시키려는 목적, overfitting 방지를 위함

  1. Model capacity 조정
    1. Hidden layer 노드 수, layer 수 줄이기
    2. Weight decay: 불필요한 weight를 0 근처로 유도(l1, l2 penalty)
    3. Early stopping: overfitting 조짐시(validation set의 cost를 모니터링) stop
  2. 더 많은 데이터 확보(data augmentation)
    1. Image의 경우: 좌우반전, 명암 등 변경
    2. Text의 경우: 한영한 번역 득
  3. 앙상블(ensemble)
  4. Drop out
    1. P 확률로 뉴론을 드랍
  5. Batch normalization
    1. Batch normalization layer를 추가하여 input을 변경(시작점 제외)

### CNN을 쓰는 이유

  1. Weight를 공유
  2. FCN에 비하여 parameter수가 감소하여 overfitting을 줄여줌
  3. Pooling
    1. Max, average

Gradient vanishing \&lt;-\&gt; Gradient exploding

# **데이터 분석 On Cloud**

클라우드 환경에서의 데이터 탐색 및 분석(AWS 환경에서의 데이터 탐색, 기초통계분석, 모델링 방법, DataBricks 활용 방법 등등)



# 1. on AWS

0. Amazon <mark>Kinesis</mark> 를 활용한 실시간 스트리밍 데이터 수집,처리 및 분석.

   - 서버리스 서비스로 관리할 서버가 존재하지 않음

   - kinesis 에서 제공하는 편집기를 통해 sql 쿼리를 구축 할 수 있음.

   - IDE 를 AWS 에 연결하여 java 라이브러리를 설치가능. 확장 가능한 라이브러리에는 스트리밍 데이터 분석에 필요한 기능들이 들어있음

     <사용사례>

     1. java 어플리케이션을 활용한 iot 스트리밍 ETL

        Java 애플리케이션을 작성하고 Amazon Kinesis Data Analytics를 사용하여 가전 제품, 내장 센서, TV 셋톱박스 등과 같은 IoT 디바이스에서 전송되는 스트리밍 데이터를 변환, 집계 및 필터링할 수 있습니다. 그런 다음, 센서가 특정 운영 임계값을 초과하는 경우 데이터를 사용하여 실시간 알림을 전송할 수 있습니다.

     2. sql 을 통한 실시간 로그 분석

        

1. Amazon <mark>SageMaker</mark> 를 활용한 머신러닝 모델 생성

   - 개발자 및 데이터 분석가가 머신러닝 모델을 빠르게 구축, 학습 및 배포할 수 있도록 하는 완전 관리형 서비스.
   - 머신 러닝에 사용되는 모든 구성 요소를 단일 도구 세트로 제공하여 반복적인 머신러닝 프로세스를 더 빠르게, 더 적은 노력으로 처리 할 수 있음.


​	**상세 기능**

 - SageMaker Studio

   -> 별도의 어플리케이션 설치 없이 주요 기능(notebook, debugger,autopilot, model monitor)을 사용가능

 - SageMaker Notebooks

   -> 'snapshot' 기능을 통해 본인이 작업한 노트북을 깃텁에 올리거나 공유가능한  URL로 저장 할 수 있음.  다른 사용자는 해당 url을 활용하여 조회하거나 편집 가능함.

- SageMaker Experiment

  1. 머신러닝 모델에 대한 반복학습 구성/축적 가능

     -> 입력 매개 변수, 구성 및 결과를 자동으로 캡쳐하고 이를 experiment로 저장하여 반복을 관리.

     -> sagemaker에서 코드로 작업하거나, 오토파일럿 연동 역시 가능하며 pandas 형태로 결과를 제공함

  2.  studio 리더보드를 통한 experiment의 결과 조회 및 비교 가능, 

     -> studio 내 결과를 제공하며 사용성이 뛰어남

  3. 다중 job(하이퍼파라미터 조정)  시행 가능

     -> 파이썬 코드형태로 사용 가능하여 유연성이 뛰어남.

- SageMaker Debugger  <mark>MLDL 이나 데이터브릭스에는 없는 기능

  -> 하이퍼파라미터 튜닝에러를 잡아줌.

  파이썬 코드로 사용하며, 디버거의 통계를 스튜디오에서 시각화하여 쉽게 이해할 수 있도록 해줌. 

  일반적인 훈련 문제가 감지 될 때 경고 및 치료 조언을 생성하며, 모델 설명 방법의 초기 단계를 나타내는 모델 작동 방식을 해석 할 수 있음

- SageMaker Autopilot

  -> 오토머신러닝 기능 제공(파이썬 코드로 사용 및 옵션설정)

  -> 전체 머신러닝 진행과정 및 결과를 파이썬 코드로 별도 제공하여 재사용성이 뛰어남.

  -> 결과 보고서가 ipynb 파일로 자동 생성, 익숙한 노트북의 형태로 제공되어 친숙함.

- SageMaker Model Monitor <mark>MLDL 이나 데이터브릭스에는 없는 기능

  -> 기존 모델의 학습에 사용되는 프레임에 맞도록 contraint 를 지속적으로 체크하며, 모델 흐름 모니터링을 진행함(데이터 변화에 따른 concept drift 감지 등등)

  

# 2. On Databricks

Azure, AWS에 있음. 통합 데이터 분석 환경을 제공함.

0.  Data Science Workspace

   - Collaborative Notebooks 기능

     -> 파이썬, R, SQL, Scala 문법을 제공하는 노트북 사용가능. 노트북을 통해 작업물을 공유 할 수 있으며  다른 사용자의 결과 조회 및 편집도 가능함.

     -> 잡 스케쥴러 기능(특정시간에 특정작업 시행), 대쉬보드 기능(sparked-dashboards를 통해 인사이트 공유), 로그 관리 및 감독 기능 제공

     -> 데이터브릭스 내부에서 Tableau, Looker, PowerBI, Rstudio, SnowFlake 와 같은 툴을 연결하여 사용할 수 있음.

   - MLR(Machine Learning Runtime) 기능 

     -> 콘다 통합환경에 설치되어 있는 패키지들이 모두 설치되어 있음.

     -> tensorflow, keras, pytorch 등등 머신러닝에 필요한 다양한 패키지들이 설치되어 있음.

   - Augmented 머신러닝 기능 제공 / 모델 관리(Managed ML Flow)

     -> 수백, 수천개의 experiment 들에 대한 트랙킹과 비교분석이 가능해짐

     -> 하나의 experiment 에 대해 모델 학습 과정(입력 매개 변수,구성, 결과 등등...)을 기록하여 관리 가능

     -> 최적의 모델탐색 기능 제공(automated model search), 하이퍼파라미터 자동튜닝 기능 제공

     

1. Unified Data Service

   - databricks delta를 활용하여 저장공간 최적화, 데이터 업데이팅, 쿼리문 실행 등 데이터 전처리에 필요한 기능들을 손쉽게 사용할 수 있음.
   - databricks delta는 아파치 스파크와 100% 연동





## 데이터 엔지니어링

(24p ~ 끝까지)

### ETL

- ETL은 Extract(추출) 하여 Transformation(변환) 하여 타겟시스템에 Load(적재) 하는 과정.
- 다양한 형식의 데이터를 한 시스템에 통일된 양식으로 모으는 작업.
- 데이터 이동과 관련된 모든 분야에 활용
  - BI 및 데이터 분석을 위한 수집
  - 마스터 데이터 관리
  - 데이터 Migration(e.g. 온프로미스에서 AWS로 서버 이전) + 빅데이터 이행과제 지원
  - 같은 데이터를 사용하는 두 시스템의 데이터 동기화
  - 기업 간 데이터 전송(e.g. 판토스의 물류이동 데이터를 화학에서 받아서 쓰기)
  - 데이터 서비스(e.g. 흩어져 있는 모든 데이터를 뽑을 수 있는 통합된 웹포탈)

- 갖추어야 할 것
  - 설계 개발이 되고, 프로그램을 실행한 것에 대한 모니터링이 가능
  - 다양한 시스템에 접근이 가능해야함.(AWS도 되고, 오라클도 되고, 엑셀 업로드도 되고...)
  - 보내는 것도 마찬가지임
- ETL 툴 선정 기준
  - 호환성(추가 변환작업없이 여러가지 포맷을 받고 여러가지 포맷으로 보낼 수 있는가)
  - 여러테이블을 join해서 하나로 만들거나 반대로 분할해주는 프로그램도 되는가?
  - GUI가 사용자 직관적인가?
  - 오류처리가 가능한가?
  - 보안이 유지되는가?
  - 한번에 여러개 돌릴 수 있는가?
  - 프로그램을 수정하면 연관된 프로그램에 얼만큼의 영향이 있는지 확인 가능한가?
  - 배치 스케줄링 할 수 있는가?
  - 소스 to 타겟 컬럼 자동매칭 (e.g. 소스와 타겟 테이블 컬럼이름이 같으면 일일히 입력안해도 자동으로 1초만에 연결되도록)
  - 작업 소요시간을 미리 알 수 있는지?



### 매핑 작업의 순서

1. 소스 시스템 분석

   1. 소스 시스템 내 테이블 분석
   2. 1에서 분석한 테이블의 컬럼 분석

2. 타겟 시스템 분석

   1. 데이터 넣을 DB구현
   2. 소스 시스템 분석을 토대로 타겟의 테이블, 컬럼 정의

3.  매핑 정의

   1. 어떤 테이블의 어떤 컬럼과 연결시킬지 결정
   2. 추출하는 횟수 설계
   3. 데이터를 변경시키는 로직도 설계하기
   4. 이 모든것을 매핑 정의서로 작성

   

### 추출 및 전송

1. 추출 전송 방식
   1. 온라인 : API나 쿼리문으로 받아오기
      - 일반적인 방식
      - 하지만 운영서버가 해당 데이터를 보내주기위한 작업을 너무 오래하면 안됨
      - 그래서 소량 전송할 때 주로 사용함
   2. 오프라인 : 파일로 받아오기
      - 운영서버에 무리가 적어서 초기적재할 때 사용.
   - 그냥 온라인으로 초기적재 하기도함(운영서버 사용 잘 안하는 새벽에)
   
2. (운영 중) 추출 방식
   1.  Refresh : 싹다 갈아엎기
      - 데이터 적은 것만, 디멘젼 같은 것만.
   2. Timestamp : 시간 비교
      - 데이터 변경일자를 확인하여 마지막 추출 일자보다 나중 것만 추출
      - 원래 소스시스템에 변경일자 컬럼이 있어야함
   3. Snapshot비교 : 데이터 적재된 것을 비교
      - 담겨있는 전체데이터를 '캡쳐'하여 추출 전 후를 비교
   4. DBMS 로그 : 변경 내역 관리
      - 로그에 변경내역만 올려둠
      - 로그를 확인하여 변경내역만 추출함
   5.  Application log : 변경 내역 관리
      - 로그 파일말고 어플리케이션에 기록함
      - 파일 저장이아니고 어플리케이션에 또 무언가를 보내야하므로 부하가 크고 복잡

### 변환

- 데이터 표기 방식 통일
  - Y/N, 0/1, T/F를 로직을 통해 하나로 변경
  - 시간 표기 방식 통일
  - NULL값을 처리하는 방식
  - 불필요하게 다양한 Key값을 하나의 Key값으로 통일
  - 컬럼에 comment달기
  - 필요하다면 Min, Avg, Sum, Count 등의 집계함수 사용
  - 계산 및 로직이 필요한 컬럼(파생변수)



### 적재

- Append : 기존 테이블에 데이터 추가하기
- Delete + Append : e.g. 최근한달 데이터를 update할 경우 한달치를 delete한 뒤에 append
- Truncate : 기존의 데이터를 싹다 날리기
- Insert : append를 DB에서 하는 방식임
- Create : 임시 테이블을 생성하면서 적재함 (툴 내에서 자동으로 이뤄지는 case임.)



- 적재할 때는 적재 시간과 용량을 고려해야한다.(Tuning과 복잡한 key구조 제거,index 활용 등)



### 검증

1. data를 못불러와서 공백이나 NULL이 들어갔는지
2. 최소 최댓값 (창고의 재고가 마이너스면 이상하잖아?)
3. 건수 검증(1:1로 불러왔는데 왜 데이터 수가 달라?)
4. 코드데이터가 제대로 들어갔는지? (고객코드는 (0001) 4자리인데 왜 1자리밖에 없어)
5. 날짜 데이터 검증 (YYYYMMDD와 YYYY/MM/DD는 엄연히 다르다.)
6. 업무 로직 검증(코드번호가 S로 시작하면 샘플로 만든 제품이다... 등등)



### 배치

1. 고려해야할 사항
   1. 소스 시스템은 언제 배치되는지?(새 데이터는 5시에 만들어지는데 4시에 가져와봤자)
   2. 데이터 간 선후행 관계 고려(분석용 마트가 디멘젼보다 먼저 적재되면 안된다.)
   3. 언제 작업할 수 있는지?
   4. 동시에 몇개가 돌아갈 수 있는지?
   5. 각 batch를 그룹핑할 수 있는지?
2. 담당자가 처음부터 끝까지 보고있지 않아도 될정도로 되어야 함
   1. 오류가 발생하면 영향이있는 후속작업이 자동으로 멈추도록 해야함
   2. 후속작업은 선행작업 바로 다음에 연결하거나, 끝나는 시간을 예측하여 진행하도록함

### 정제

- 정제는 되도록 소스 데이터를 고칠 수 있도록 한다.
- 변환을 써서 고치면 당장은 맞겠지만, 로직이 복잡해지고 오랜시간 후에 소스가 고쳐졌을때 문제가 발생
- 바꿀 수 없다면 ETL에서 진행한다.
- 품질 관리 프로세스
  1. 데이터에 문제가 없는가?
  2. 약간 있는 문제가 큰 영향이 있는가?
  3. 이 데이터 담당부서는 어디고, 필요없는 부분은 있고, 무엇에 쓰이며, 바꾸는 비용은 얼마나 되는가?
  4. 바꿀 방법을 모색한다.
  5. 바꾸고 난 뒤 영향도에 따라 후속작업을 진행한다.
  6. 다시 품질확인을 하여 문제가 없는지 확인한다.
- (정치적인 방식)
  - 운영계 시스템에서 데이터에 대해 품질을 검증하고 개선할 수 있도록 해야한다.
  - 데이터가 이상함을 현업에게 고의로 노출하여 문제가 있음을 보여준다.
  - 최대한 소스쪽 수정을 요구하고, 정말 부득이한 경우에만 ETL에서 수정

  ## 알고리즘 이론정리

### 회귀분석

회귀분석에서, 회귀계수값을 제대로 추정하고 이를 신뢰하기 위해서는 필요한 몇가지의 가정들이 있음. 

1. <mark>모델의 형태에 대한 가정</mark>

   Y는 X와 파라미터들의 선형 결합으로 이루어진 형태이다.

2. <mark>에러(잔차)에 대한 가정

   - 에러 e1,e2..en이 iid(identically and independently) 하게 평균이 0이고 분산이 sigma^2인 정규분포를 따름

     -> <mark>에러의 정규성</mark>: 에러는 정규분포를 따라야 함

     -> <mark>에러의 등분산성</mark>: 에러는 constant 하여야 함 (증가하거나 감소하는 trend 가 존재해서는 안됨)

     -> <mark>에러의 자기상관 부재</mark> 에러들끼리는 서로 독립이어야 함

     -> 이때 해당 가정에서 정규성 가정을 만족하지 않아도 회귀계수 추정값은 unbiased estimator임.. 이때 비교대상을 선형이면서 Unbiased 추정량들의 집합으로 한정할 경우에, Least square estimator 는 불편추정량이자 최소 분산 (smallest variace) 을 지닌 최량선형불편추정량(Best Linear Unbiased Estimator)임 (가우스-마코브정리)

     불편추정량 - 편향되지 않은 추정량

     <mark>LSE(Least Square Estimator)</mark> -최제추정량이란?

     - 회귀계수 추정방법으로, 최소제곱법을 이용하여 잔차의 제곱합이 최소가 되도록 함. 

3. <mark>predictors 에 대한 가정

   - 설명변수들은(predictor variables) 모두 non-random이다

     -> 따라서 반응변수의 정규성은 에러의 정규성에서 기인함

   - 설명변수의 관측값 xij에는 error 가 존재하지 않는다

   - 설명변수 x1,x2,,,xp는 서로 독립이다.

**다중공산성(Multicollinearity)**

- **독립변수가 다른 독립변수들의 선형 결합으로 나타낼 수 있을때**(즉, 독립변수들 사이에 서로 강한 연관관계가 존재할때) 발생하는 문제로, 이 경우 X tranpose x X 행렬의 역행렬이 존재하지 않는 문제가 발생하며 **회귀계수 추정값을 LSE로 계산할수 없는 문제가 발생**함.

  -> 다중공산성 문제가 발생할 경우, 개개 변수들이 반응변수에 미치는 Unique 한 영향도를 측정하는것이 불가능해 지며, 예측된 회귀계수값이 데이터의 변화에 매우 민감하게 변하는 문제가 발생함. 즉 **모델 일반화가 힘들어짐**.

  즉 예측한 회귀계수의 sampling error 값이 매우 커지기 때문에 해석과 예측 두가지 경우가 모두 힘들어짐

- 다중공산성 문제는 모델링 에러는 아니고, 대체로는 <mark>데이터의 부족 문제</mark>로 인해 발생함

- 다중공산성은 Variance Inflation Factor, **VIF** 를 통해 파악 할 수 있으나 모든 다중공산성을 표현해 주지는 않음

  -> VIF 는 1/1-R^2로 표현되며, 통상적으로 vif가 10 이상일때 변수에 다중공산성이 존재한다고 파악하나 10이라는 숫자에 통계이론적인 근거는 찾기 힘듬

추가적으로, 회귀분석의 계수에 대해 해석할때는 "when other factors are constant" 라는 가정이 필요함. <mark>다른 변수가 고정적일때를 가정하지 않고는 해당 변수가 회귀계수만큼 반응변수에 영향을 미친다고 해석 할 수 없음</mark>



### Generalized Linear Model(GLM)

numeric 변수만을 반응변수로 하는 회귀분석의 일반화된 모형으로, link function 의 종류에 따라 다양한 모델이 존재함.

GLM 은 3가지 부분으로 구성되어 있는데, random component 인 반응변수 Y, systematic component 인 설명변수 X, 그리고 Y와 X사이의 관계를 규명하는 link function임. 근데 여기서 link function 이 되기 위한 조건이 두가지 존재하는데,바로 monotone(단조증가) 하고 differentiable(미분가능) 하다는 점임. 이 두가지 조건을 만족하지 않은 함수는 link function 이 될 수 없음.

1. Poisson regression

   반응변수가 count data 일때 주로 사용함, 반응변수의 평균과 분산이 모두 모두 m(모수) 로 일정하며 link function g(m) = log(m) 임 

   실제로 데이터의 분산(m)이 평균값(m) 보다 크게 될 때 포아송 회귀를 사용하게 되면 Overdispersion 문제가 발생하게 되며, 이는 데이터들의 이산성으로 인해 쉽게 발생하는 문제임

   -> 이 경우에는 Y의 평균과 분산값이 다르지만 동일한 link function 을 사용하는 Negative Binomial 회귀를 사용하면 해결됨.  NB 회귀는 Y가 NB(m,k) 를 따른다고 가정하며, 이때 분산 k는 m + Dm^2으로 표현됨. D가 0으로 수렴함에 따라 overdispersion 문제가 발생하지 않는다고도 볼 수 있음. 

2. Logistic regression

   반응변수가 Binary 일때 사용하는 회귀로, link function 으로 sigmoid 를 사용함. logit(p) = log(p/1-p) = a+bx. p는 성공확률

   다른 변수들이 constant 할 때, x가 1단위 증가할때 마다 오즈가 exp(beta) 배 증가한다고 해석. 베타=0 이면 x가 변화해도 오즈 변화 없음.

3. Multicategory Logit Model

   반응변수가 binary 이상의 범주를 가지고 있는 변수일때 사용하며 nominal일 경우 baseline category logit을, ordinal 의 경우 cumulative logit 을 사용함

   - baseline category logit

     -> Y1,Y2..Yn이 iid 하게 multinomial(1,p1,p2..pJ) 을 따른다고 가정함. J는 카테고리의 갯수. 마지막 카테고리 J가 baseline이 된다고 할 때, baseline category logit 은 log(pj/pJ), j = 1,2,...J-1 로 표현 가능.

     즉, log(pj/pJ) = aj + Bjx 이며 J=2 일땐 로지스틱 회귀 모형과 동일해짐. 

     베이스라인 카테고리 로짓 모형은 카테고리를 짝지어서 비교하는 형태로 총 J-1개의 식이 동시에 피팅되게 되며 각각의 피팅에 비교하여 SE가 줄어듬.  

   - Cumulative logit 

     -> 반응변수의 카테고리가 ordered 된 경우에 로짓에 순서 정보를 반영할수 있고, 베이스라인 로짓 모델에 비해 해석이 간단하며 성능이 좋음.

     logit(P(Y<=j)) = log(P(Y<=j)/1-P(Y<=j)) = log[(p1+p2..pj) / (p1+p2...pJ)] = aj + Bx

     베이스라인 모형과는 다르게 변수 x가 반응변수에 미치는 영향이 J-1개 모두 B로 동일함. 즉 추정되는 베타 계수가 한개인데 이를 적용하기 위해서는                                                        각각의 cumulative 확률에 같은 확률 베타가 적용된다는 proportional odds 가정이 필요함(확률 오즈 모델과 j번째 카테[고리에 대한 분리 모형 Bj와 비교함)

GLM 모델들을 서로 비교할때는 Deviance 를 이용함. 

H0: 현재모델 유지 H1: full model

Lm, Ls를 각각의 모델에 대한 maximized log-likelihood라고 할때, GLM의 deviance 는 -2(Lm-Ls) 로 표현 가능하며 이를 LR statistic 이라고 함. 해당 통계량은 자유도가 두 모델의 파라미터 갯수 차이인 카이제곱 분포에 근사하며, 값이 클수록 귀무가설을 기각. 



### 나이브베이즈 분류기(Naive Bayesian Classifier)

스팸메일 필터, 텍스트 분류, 감정분석, 추천시스템 등에 활용되는 분류 기법으로 . 특정 개체 x가 특정그룹 c에 속할 확률 즉 p(c|x) 이하 사후확률을 구하는 것이 목적임. p(x|c) 는 특정그룹 c인 경우에 특정 개체 x가 그룹에 속할 조건부 확률이며, likelihood임. p(c)는 특정 그룹 c가 발생할 빈도, 즉 클래스 사전 고유 확률(prior probability)라고 함.

**특징**
  1. 지도학습
  2. feature에 따라 라벨을 분류함에 있어 베이즈 정리 사용

**가정**
  1. 모든 feature가 서로 독립

**목적**
  1. x 가 특정그룹 c에 속할 확률, p(c|x)

사후확률 ~ Likelihood x 사전확률 로 요약표현가능

나이브베이지안은 

- multi-class 분류에서 쉽고 빠르게 예측이 가능함

- 설명변수들이 모두 독립이라는 가정이 만족한다면, 여타 다른 분류기들(로지스틱,,) 에 비해 훨씬 성능이 좋으며 학습데이터도 적게 필요함.

- 수치형(numeric) 데이터들보단 범주형(categorical) 데이터에 특히 효과적임.

- computation cost가 적기 때문에 매우 빠름.

- train 데이터에는 존재하지 않고 test 데이터에만 존재하는 범주에 대해서는 확률이 0이 되기 때문에 정상적인 예측이 불가능함.

  -> 이것을 zero frequency 라 부르며, 이를 해결하기 위해선 라플라스 추정을 이용하여 smoothing 을 하여야함.

- 설명변수 독립 가정이 성립하지 않을시에는 에러 발생 가능성이 매우 높아져서 결과를 신뢰할 수 없음.

# Ensemble Model

## 1. 소개

### 1.1 정의

  - 단순다수결 : 배깅, 랜덤포레스트에서 사용.
  - 가중다수결 : 부스팅에서 사용(가중치 다수결)
* 좀 더 robust한, 예측모형/분류모형 생성 및 feature selection(중요변수선택)을 위해 사용함
* 분류모형, 예측모형에 모두 사용가능함. 일반적으로 분류모형에 사용됨.

### 1.2. 앙상블모형의 성공요인 :: "Diversity"의 확보

* 앙상블 집합 내에 구축되는 모형이 서로 유사하지 않고 매우 다양해야 함.

* 다양한 B개의 분류기를 통합해야 데이터 자체에 존재하는 복잡/난해한 구조의 분류문제를 해결할 수 있다. 

* 각각의 샘플 데이터마다 반드시 똑같은 방식의 모형을 쓸 필요는 없음

  #### (1) 데이터의 다양성 :: Bootstrap

  * 하나의 데이터를 이용해 다양한 데이터를 만들어 내는 방법
  * 훈련데이터 전체가 L이라고 한다면, 부트스트랩 방법은 L1, L2 ..., Lb개의 데이터를 생성

  #### (2) 모형생성의 다양성 :: 분할방법

  * 각각의 훈련데이터마다 다양한 모델을 적용하는 방법도 있으나, 보통은 Decision Tree를 사용하되, Model 적합시 분할방법에 변화를 주어 모형이 다양해지도록 함.
  * 랜포의 경우 중간 노드에서 분할후보점을 선발할 때 전체변수가 아니라 임의의 변수 부분집합 중에서 선발하고, 그 중에서 분할개선도를 최대화시키는 분할 point를 탐색함.
  * 동일한 모형생성 방식을 사용하더라도 하이퍼파라미터 튜닝을 한다면 모형생성의 다양성을 확보하는 한가지 방법이 될 수 있음 

## 2. 종류

### 2.1 Bagging :: Bootstrap Aggregating

* (1) 먼저 Train Data $$L$$을 뽑는다. (ex. 전체 데이터의 70%) 
   :: $$ L=\{(x_i,y_i), i = 1, ..., n\} $$ 

  (2) $$L$$로부터 복원추출하여 부트스트랩 데이터를 B번 만든다 (일반적으로 B로 50을 많이 사용. )

   :: $$L_1, L_2, ... , L_B$$
  *** 데이터를 복원추출 시 B의 횟수를 아무리 늘려도 전체 데이터의 63.2%만 추출이 됨.
  왜 63.2%인지. 자세한 내용은 검색을 통해 확인가능(keyword : bootstrap 0.632)

  (3) 각 부트스트랩 데이터마다 모델(분류기)를 생성
  :: $$T_1, T_2, ... , T_B $$ - 여기서 T는 Decision Tree

  (4) B개의 모델을 결합시켜 최종 앙상블모델을 생성. 
  :: 범주형 모델(분류기)인 경우에는 다수결 방식에 따라 결정.
  :: 연속형 모델(Regression)인 경우에는 평균값을 사용

* 배깅방법은 데이터가 쪼금만 변해도 결과에 많은 변화가 생기는 불안정한 분류모델인 경우,
   예측력을 획기적으로 향상시킨다고 알려져 있음.
  -> Decision Tree의 경우 pruning(가지치기)를 사용하지 않는 최대 Tree를 사용하는 것이 예측정확도가 좋다고 알려짐 (가지치기를 사용하지 않은 경우에 더 불안정한 모델이므로)

* <mark>Bagging 적용하면 안 되는 경우
  1) Too small : 표본데이터가 작은 경우 전체를 잘 반영하지 못함..
  2) Noise : 데이터에 noise가 많은 경우(ex. outlier가 크게 왜곡시킴)
  3) Dependency : 데이터의 독립성 가정

### 2.2 Boosting

(https://www.youtube.com/watch?v=GM3CDQfQ4sw)

- 배깅과 마찬가지로 B개의 분류기를 생성하여 종합하는 방법이지만, 분류기를 생성하는 방식과 종합하는 방식이 조금 다르다
- 무작위로 선택하는 것보다 약간 가능성이 높은 규칙(weak learner/classifier)들을 결합시켜 보다 정확한 예측 모델을 만들어 내는 것을 말함. 
- 부스팅에 사용되는 분류기는 오분류율 기준으로 랜덤하게 예측하는 것보다 약간이라도 좋은 예측 Model 이기만 하면 효과가 있다고 알려짐. ==> 예측력이 약한 분류모델을 결합하여 강한 예측 모델을 만드는 과정!
- Adaboost 알고리즘( Adaptive Boosting)을 주로 사용함
- Boosting은 새로운 learner(classifier)를 학습할 때마다 바로 이전 결과를 참조하는 방식.
  최종적으로 weak learner로부터의 출력을 결합하여 더 좋은 예측율을 갖는 strong learner를 만들어 냄.

_____ Boosting 사례) (이 부분은 '쉽게 읽는 머신 러닝(https://blog.naver.com/laonple)'에서 참고.)

- 예시) 가령 스팸 여부를 가릴 수 있는 방법을 개발한다고 해보자. 스팸 여부 판정할 수 있는 기준은 많으며, 다음과 같은 것들이 있다고 하자. (간단한 규칙들에 대하여 Yes/No로만 판정)

  - 링크만 있는 경우 => 스팸
  - 메일 내용에 “당신의 보험료가 xxx 입니다”라는 내용이 들어 있는 경우 => 스팸
  - 도메인 주소가 확실한 경우 => 스팸 아님
  - 보낸 사람이 확실한 경우 => 스팸 아님

  스팸 여부를 판단할 수 있는 기준들은 위에서 열거한 일부 경우뿐만 아니라 매우 많은 기준이 있을 수 있다. 그런데 위에서 열거한 기준들은 모든 메일에 똑같이 적용할 수 있을 정도로 확실한/강력한 규칙은 아니다. 이런 것들을 weak learner(rule, classifier)라고 한다. 무작위로 선정하는 것보다는 성공 확률이 높은, 다시 말하면 오차율이 50% 이하인 학습 규칙을 말한다


  Weak learner를 strong learner로 바꿀 수 있는 방법은 무엇일까? 간단한 방법은 다음과 같다.

  - 평균/ 가중 평균(weighted average)를 사용하는 방법
  - 가장 많은 의견(vote)을 얻은 것을 선정하는 방법
    
    

  앞서 살펴본 스팸 메일 여부를 가리는 규칙에 위 두가지 방법을 적용하면, 좀 더 강력한 스팸 필터가 만들어질 것이며, 규칙이 더 늘어나면 늘어날수록 더 좋은 결과를 기대할 수 있게 된다.

### 2.3 <mark>Bagging vs. Boosting

- Bagging은 모든 Bootstrap이 서로 독립적인 관계를 갖는다. 하지만 Boosting은 순차적으로 처리가 되며, 에러가 발생하면 그 에러의 weighting을 올리기 때문에 현재의 weak learner가 이전 weak learner의 영향을 받는다.
- Boosting은 최종적으로 weighted vote를 하지만, Bagging은 단순 vote를 한다.
- Bagging은 Variance를 줄이는 것이 주 목적이지만, Boosting은 Bias를 줄이는 것이 주 목적이다. 
- 잡음이 없는 데이터에 대해서는 Boosting이 Bagging보다 우수하다.
- Bagging은 overfitting 문제를 해결할 수 있지만, Boosting은 overfitting 문제로부터 자유롭지 못하다
