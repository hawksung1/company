# **2020 TCT**  **주스킬**  **-**  **빅데이터 대비**

24문항, 200분

| 분류 | 소분류 | number | Score |
| --- | --- | --- | --- |
| 분석시나리오 정의 | ㅇ | 3 | 24 |
| 분석 방법론 구체화 | ㅇ | 3 | 24 |
| 데이터 전처리, 마트구성 | 데이터 전처리파생변수 생성분석 마트 구성 | 3 | 24 |
| 데이터 탐색 | 시각화분포이해 | 2 | 5 |
| 기초 통계 분석 | 추정, 검정샘플링기본 개념다변량 | 3 | 7 |
| 모델 이해 | ML / DLText 분석 | 5 | 19 |
|
 | 모델 진단, 평가모델 적용, 활용 | 2 | 5
 |
| 데이터 분석 cloud | AWS 환경 data 탐색기초 통계모델링Data Bricks 활용 | 3 | 8 |

Cloud: AWS 서비스 중 분석

DAP MLDL 에서 서비스

AutoML

AWS Technical Essential 강의, 클라우드 환경에서 쓰는 분석기능(PySpark 등)

서술형: 1. Cloud에서 분석 방법론 구체화 문항(Cloud 환경 특이점(탐색, 모델링 관점)을 명시해 서술

2. 데이터 엔지니어링(교재 참조)

# **2019** 년 기출 객관식 키워드

트리에 기반한 모델에서 얻어지는 예측 값은 학습데이터에 존재하는 관측 값의 범위 안에만 있음.

**Bayesian Posterior interval** = 베이즈 신용 구간

    신용 구간이 사전 분포와 문맥 정보 혼합
    Prior 분포(사전 분포): 미리 알고 있던 모수의 분포
    Margin of error(허용 오차):

**빈도주의 신뢰 구간**

    신용 구간이 데이터에만 의존

### SVM

    분류를 위한 선(면)을 정의(마진이 최대화 되는)
    Non support vector: svm에 사용되지 않은 vector(있던 없던 상관 없음)

Bias, variance trade-off: 오버피팅-bias 감소, variance 증가

### **Poisson Regression** 
    
    평균과 분산이 같아야 함. 단위사간에 사건이 몇번

Gaussian 모델(정규 분포):

### 이산 확률 분포

    Binomial 모델(이항 분포): 1번 실행시 베르누이 분포, 성공/실패
    Negative Binomial 모델(음 이항 분포): k번 성공 확률 분포
    Multinomial: 주사위
    Geometric: 베르누이 시행을 성공할때까지의 확률변수의 분포

Boosting: 가중치

Bagging: 투표

### 로지스틱 회귀 추정
    likelihood 최대화, MLE(Maximum Likelihood Estimation)
    likelihood: 독립 사건 = 확률, 연속 사건 = PDF 값

### Ordinal logistic regression(카테고리에 순서가 있을 때 사용)

    Proportional odds model: log odds와 독립변수 사이의 선형성을 가정,
        Proportional odds assumption: 1,2,3 이 있을 때 1,2 거리, 2,3의 거리가 같다.

Multinomial logistic regression: log odds와 독립변수 사이의 선형성을 가정

### 카이제곱 검정

    기본 가정: n(빈도) \&gt;= 5, 관측치 서로 독립.

### **SGD**(Stochastic Gradient Decent): mini batch에 대해 loss function을 계산, batch size 등가시 BGD랑 차이가 적어짐.

    장: local minima에 빠지지 않음
        빠름

    단: 부정확

BGD(Batch Gradient Decent): Batch 전체에 대해 loss function 계산

Vanishing gradient: 레이어가 적으면 발생했다고 할 수 없다.

OLS(Ordinary Least Squares): 결정론적 선형 회귀. RSS 최소화.

    Blue(Best Linear Unbiased Estimator): Unbiased, 가정 효율적 추정방법.

        1. 선형성
        2. 오차항 평균 0
        3. 동분산
        4. 오차항 독립
        5. 독립변수가 주어진 것이라는 가정

GLS(Generalized Least Squares)

Naive bayes: 결측치를 처리하지 않고도 사용 가능

#  **2019** 년 기출 서술형 키워드

A+B = C, 총 4시간, 5분마다 데이터 측정, 하루에 4번, 배치마다 온도, 압력, 유량등을 조절

변수 개수 \* 시간 개수 > 배치 수 = PLS else 뉴럴, 랜포 등

### PLS(부분 최소 제곱)

    1. 예측 변수가 매우 공선적이거나 예측 변수의 수가 관측치의 수보다 많으며 범용 최소 제곱법에서 계수를 생성하지 못하거나 표준 오차가 높은 계수를 생성할 경우에 특히 유용
    2. 다중 회귀 분석과 달리 예측 변수가 고정되어 있다고 가정하지 않음
    3. 즉, 예측 변수를 측정할 때 오류가 발생할 수 있지만 불확실성을 측정하는 데 더 로버스트

서술형 서술 순서

  1. 데이터 자체의 분석(독립성 검정, 종속변수 설정 등)
  2. 모델의 선정(지도, 비지도 등), 그 이유
  3. 데이터 전처리(na 값 처리, 표준화, 영향력 파악)
  4. 모델 학습 방법
  5. 모델 검정(cross validation 등)
  6. 예측

배치가 다 돌지 않은 상태에서의 예측

    지금 데이터를 기반으로 배치가 끝났을 때의 데이터 값을 예측하여 그 데이터를 기반으로 예측

# **2018**  객관식 기출 키워드

SARIMA 모형: Seasonal ARIMA

Stationary time series(안정시계열) 데이터에 적ㅇㅇ 가능하므로 안정성을 만족

    1. 평균이 일정할 것
    2. 각 시점별 분산이 동일항 것
    3. 시점간 공분산이 시차에만 의존할 것

Naïve Bayes: X 독립을 가정, 결합확률분포가 각 X변수들의 확률 곱으로 표현

포아송: 단위시간당 발생한 사건 빈도, 평균과 분산이 같아야 함. 동일하지 않을경우 음이항 분포를 가정

GLM 모델: Generalize Linear Model,

독립변수와 종속변수 사이의 선형성,

오차항의 정규성

독립성

등분산성

4가지를 만족해야 하는 기본적인 회귀모델과는 다르게

오차의 정규성, 등분산성을 만족하지 않아도 됨

종속변수의 분포가 정규분포인 경우: gaussian

이할분포인 경우: binomial

포아송 분포인 경우: poisson

역정규 분포인 경우: inverse gaussian

회귀분석, 분산분석: 종속변수가 정규분포되어 있는 연속형 변수

음이항 분포: k번 성공할때 까지 시행(\&lt;-\&gt; 포아송 분포)

K medoid:

부트스트랩 :

    원래의 모집단이 iid 가정을 충족
    재표집의 수가 적을 경우 아웃라이어의 영향 받을 수 있음
    분석 할때마다 결과가 조금씩 다름(대표집 수를 5000회 이상시 무시)

    Idd
        Independent and identically distributed
        확률변수가 여럿 있을 때 서로 상호독립, 동일한 확률분포
    중심극한 정리에 의해 parameter 추정치들이 평균은 참값으로 수렴
    모집단의 분산이 큰 경우 부표본의 반복추출 횟수, 크기를 늘려야 함
    부표본 parameter 추정치들은 정규분포를 따름

Content based filtering: 단순 연관 추천.

Collaborative filtering: 행동양식 기반, 다른 유저와 비교하여 추천

Pearson 상관계수: 교호작용을 고려하지 않음

    교호작용: 한 요인의 효과가 다른 요인의 수준에 의존하는 경우

PCR(Principal Component Regression)

    독립변수만을 고려하여 주성분을 추출

PLSR(Partial Least Square Regression):

    원본 데이터 대신 상관 관계가 없는 성분 집합으로 예측변수를 줄이고 이러한성분에서 최소 제곱법을 수행하는 방법.
    독립변수와 종속변수의 상관관계를 고려하여 주성분을 추출
    단: 예측 변수가 고정되어 있다고 가정하지 않기 때문에 오류 발생

F통계량: 모형이 통계적으로 유의미 한가 체크

T통계량, p val, 신뢰구간 확인: 회귀 계수들이 유의미 한가

결정계수: 모형이 얼마나 설명력을 지니는가

VIF, 다중공산성 여부

Normal QQ plot: 정규성 가정에 대한 검토

Epoch = all data

All data /batch size = n iteration

# **2018**  **서술형 기출 키워드**

예측방범론 -> 모델 설계 바닥부터(변수 설명, 모델 선정 등등)

# 데이터 엔지니어링

### ETL

1. Extract, Transformation, load - 데이터를 추출 변환하여 타겟시스템에 적재
2. 다양한 형식의데이터를 한 시스템에 통일된 양식으로 모으는 작업
3. 데이터 이동과 관련된 모든 분야에 활용

    1. BI 및 데이터 분석을 위한 수집
    2. 마스터 데이터 관리
    3. 데이터 이관 + 빅데이터 이행과제 지원
    4. 같은 데이터를 사용하는 두 시스템의 데이터 동기화
    5. 기업 간 데이터 전송
    6. 데이터 서비스
4. 갖추어야 할 것

    1. 설계 개발이 되고, 프로그램을 실행한 것에 대한 모니터링 가능
    2. 다양한 시스템에 접근이 가능해야 함

5. ETL 툴 선정 기준

    1. 호환성
    2. 여러 테이블을 join, split 가능해햐
    3. GUI 직관, 오류처리, 보안, 멀티(여러개 동시)
    4. 배치 스케줄링, 타겟 컬럼 자동매칭, 자업 소요시간 표시
### 매핑 작업 순서
  1. 소스 시스템 분석
    
    1. 소스 시스템 내 테이블 분석
    2. 위에서 분석한 테이블 컬럼 분석
  2. 타겟 시스템 분석

    1. 데이터 넣을 DB구현
    2. 소스 시스템 분석을 초대로 타겟의 테이블, 컬럼 정의
  3. 매핑 정의
  4. 어떤 테이블의 어떤 컬럼과 연결시킬지 결정
  5. 추출하는 횟수 설계
  6. 데이터를 변경시키는 로직도 설계
  7. 이 모든것을 매핑 정의서로 작성

### 추출 및 전송

  1. 추출 전송 방식
    1. 온라인: API나 쿼리문으로 받아오기
      1. 일반적인 방식
      2. 하지만 운영서버가 해당 데이터를 보내주기위한 작업을 너무 오래하면 안됨
      3. 따라서 소량 전송시 주로 사용
    2. 오프라인: 파일로 받아오기
      1. 초기 적재시 사용, 운영서버에 무리가 적다
  2. (운영중)추출 방식
    1. Refresh: 싹 갈아엎기
      1. 데이터가 적은 디멘젼 등에 적용 - 디멘젼(dimension): 비즈니스 이벤트에 독립적인 데이터
    2. Timestamp: 시간 비교
      1. 데이터 변경일자를 확인하여 마지막 추출 일자보다 나중 것만 추출
      2. 원래 소스 시스템에 변경일자 컬럼이 필요
    3. Snapshot 비교: 데이터 적재된 것을 비교
      1. 담겨있는 전체 데이터를 캡쳐하여 전후를 비교
    4. DBMS 로그: 변경 내역 관리
      1. 로그에 변경 내역만 올려둠
      2. 로그를 확인하여 변경내역만 추출
    5. Application log: 변경 내역 관리
      1. 로그 파일말고 어플리케이션에 기록
      2. 파일 저장이 아니라 어플리케이션에 부하가 크고 복잡

### 변환

  1. 데이터 표기 방식 통일
    1. Y/N, 0/1, T/F를 로직을 통해 하나로 변경
    2. 시간 표기 방식 통일
    3. NULL 값을 처리하는 방식
    4. Key값 통일
    5. Comment
    6. 집계함수(Min, Avg, sum 등)을 사용
    7. 파생변수(계산 및 로직이 필요한 컬럼)

### 적재

  1. Append
  2. Delete + append
  3. Truncate: 기존 데이터 싹다 날림
  4. Insert: append를 DB에서 함
  5. Create: 임시 테이블을 생성하여 적재(툴내 자동 적용)
  6. 적재할 때는 적재 시간과 용량을 고려

### 검증

  1. Data를 제대로 못 불러와 공백,null 등이 생겼는지
  2. 최소 최댓값
  3. 건수
  4. 코드 데이터가 제대로 들어갔는지
  5. 날짜 데이터 확인
  6. 업무 로직 검증

### 배치

  1. 고려해야 할 사항
    1. 배치 시각
    2. 데이터간 선후행 관계
    3. 동시에 가능한 개수
    4. 배치 그룹핑
  2. 담당자가 없어도 상관 없어야 함
    1. 오류가 발생하면 영향있는 후속작업이 자동으로 정지
    2. 후속 작업은 선행작업 바로 다음에 연결 OR 끝나는 시간 예측하여 진행

### 정제

  1. 정제는 되도록 소스 데이터를 수정
  2. 변환을 사용하면 로직이 복잡해지고 후에 소스 수정시 문제가 자주 발생
  3. 바꿀 수 없다면 ETL에서 진행 ?????
  4. 품질 관리 프로세스
    1. 데이터에 문제가 ㅇ벗는가
    2. 약간 있는 문제가 큰 영향이 있는가
    3. 데이터의 담당부서, 필요없는 부분, 어떠한 데이터, 데이터 변경 비용, 사용처
    4. 바꿀 방법
    5. 바꾸고 난 뒤 영향도에 따라 후속 작업 진행
    6. 다시 품질 확인
  5. 정치적인 방식
    1. 운영계 시스템에서 데이터에 대해 품질을 검증하고 개선
    2. 데이터가 이상함을 현업에게 고의로 노출
    3. 최대한 소스쪽 수정을 요구, 부득이한 경우만 ETL에서 수정
    4.

# **MLDL**

### SBP
  데이터 저장/처리 플랫폼
  주요기능

    1. 대용량 수집 & 전처리
      1. apache NiFi 적용
      2. 웹 기반 GUI, workfloe 작성, 제어, 피드백, 모니터링 편의성
    2. 데이터 저장
      1. 로컬 스토리지: HDFS
        1. 분산 파일 시스템
        2. 처리 관리 모니터링
      2. 메시지 큐: Kafka
        1. 실시간 데이터 스츠림 전처리
      3. 클라우드 스토리지: AWS S3, MS Azure, Google CloudStorage
        1. HDFS에 비해 확장성, 관리 편의성 UP BUT 성능 DOWN
        2. 잔순 데이터 저장에 주로 사용   
    3. 데이터 처리
      1. SQL 기반 처리
      2. In Memory 기발(Spark + Yarn)
      3. 실시간 Stram 처리
      4. NoSQL 처리(Hbase, Phoenix)

**NoSQL 적합 요건**

  1. 대규모 정형, 반정형 데이터에 대한 효율적인 random read/write 수행
  2. 수십 TB에 대한 초당 수천만 건의 처리 수행 
  3. 조회 패턴이 잘 알려져 있고 단순한 경우 
  4. 대규모 데이터 저장이 필요하고, 데이터 증가에 따라 확장이 필요한 경우

![platform_compare](./img/platform_compare.png)      


# **AI**

넓은 의미의 ai: 자동화 프로그램 모두

좁은 의미의 ai: 인간의 사고 행동 표방

딥 러닝은 머신러닝에 포함됨, 머신러닝은 넓은의미의 ai에 포함

Knowledge based ai: 사람이 자신의 인사이트로 end to end rule설정

ML based ai: data를 기반으로 인식, 예측 rule

Representation learning: data based feature creation

Deep learning based ai: neural network, all auto

정형 데이터: 엑셀 등

비정형 데이터: 이미지, 영상, 소리 등

### Ai 구현 process

  1. 전문가 시스템
    1. 모든 rule을 사람이 결정
  2. 전통적인 ml 기법
    1. Feature를 사람이 결정하여 rule은 모델링
  3. Ml/dl 기법
    1. Raw data 부터 rule 까지 모델링

Tensor: scalar -\&gt; vector -\&gt; matrix -\&gt; tensor(3d)

Fnn: feedforward neural network

Cnn: convolutional neural network

Rnn: recurrent neural network

Sigmoid: gdp 발생, 계산 복잡

Tanh: gdp 발생

Relu: 굳

Gradient descent

Back propagation

Hyperparameter: traning 동안 학습되지 않음

Learning rate

Data: 4000, batch size=400 ---\&gt;\&gt; 1epoch = 10 iteration

검출율: tp/(tp+fn) --\&gt; 불량 검수 빡세게

정밀도: tp/(tp+fp) --\&gt; 불량 검수 자체를 정제


### Loss function

Quadratic

장: 실수에도 사용 가능

단: sigmoid와 사용시 수렴이 느림

Cross entropy

이진 분류에서 사용

장: quadratic에 비해 모델 수렴

Negative log likelibood

Multiclass 에서 사용

장: softmax와 사용ㅅ 수렴이 빠름


### Optimizer

Momentum: 과거의 파라미터 업데이트 내역을 누적

Adaptive: 파라미터 업데이트가 진행될수록 learning rate를 줄임


### 가중치 초기화(weight initialization)

Lecun normal: relu 나오기 전, 가우시안 분포에서 분산을 x의 원래 분산 정도로 보정

Xavier: relu 이후, 입력/출력 노드수를 고려하여 초기값 설정

He: xavier 분산값을 2로 곱함


### Regularization

Generalization error를 감소시키려는 목적, overfitting 방지를 위함

  1. Model capacity 조정
    1. Hidden layer 노드 수, layer 수 줄이기
    2. Weight decay: 불필요한 weight를 0 근처로 유도(l1, l2 penalty)
    3. Early stopping: overfitting 조짐시(validation set의 cost를 모니터링) stop
  2. 더 많은 데이터 확보(data augmentation)
    1. Image의 경우: 좌우반전, 명암 등 변경
    2. Text의 경우: 한영한 번역 득
  3. 앙상블(ensemble)
  4. Drop out
    1. P 확률로 뉴론을 드랍
  5. Batch normalization
    1. Batch normalization layer를 추가하여 input을 변경(시작점 제외)

### CNN을 쓰는 이유

  1. Weight를 공유
  2. FCN에 비하여 parameter수가 감소하여 overfitting을 줄여줌
  3. Pooling
    1. Max, average

Gradient vanishing \&lt;-\&gt; Gradient exploding